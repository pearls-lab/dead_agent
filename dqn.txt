pygame 2.1.3 (SDL 2.0.22, Python 3.11.1)
Hello from the pygame community. https://www.pygame.org/contribute.html
Using cpu device
Wrapping the env in a DummyVecEnv.
Testing algo: dqn
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -86      |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 762      |
|    time_elapsed     | 0        |
|    total_timesteps  | 388      |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.298    |
|    n_updates        | 71       |
----------------------------------
Eval num_timesteps=500, episode_reward=-128.97 +/- 84.95
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -129     |
| rollout/            |          |
|    exploration_rate | 0.92     |
| time/               |          |
|    total_timesteps  | 500      |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.888    |
|    n_updates        | 99       |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -121     |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 737      |
|    time_elapsed     | 1        |
|    total_timesteps  | 776      |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 168      |
----------------------------------
Eval num_timesteps=1000, episode_reward=-90.97 +/- 59.33
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -91      |
| rollout/            |          |
|    exploration_rate | 0.84     |
| time/               |          |
|    total_timesteps  | 1000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.296    |
|    n_updates        | 224      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -88.5    |
|    exploration_rate | 0.814    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 686      |
|    time_elapsed     | 1        |
|    total_timesteps  | 1164     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.19     |
|    n_updates        | 265      |
----------------------------------
Eval num_timesteps=1500, episode_reward=-30.97 +/- 26.83
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -31      |
| rollout/            |          |
|    exploration_rate | 0.76     |
| time/               |          |
|    total_timesteps  | 1500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.48     |
|    n_updates        | 349      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -84.1    |
|    exploration_rate | 0.752    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 687      |
|    time_elapsed     | 2        |
|    total_timesteps  | 1552     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.885    |
|    n_updates        | 362      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -72.5    |
|    exploration_rate | 0.69     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 696      |
|    time_elapsed     | 2        |
|    total_timesteps  | 1940     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.886    |
|    n_updates        | 459      |
----------------------------------
Eval num_timesteps=2000, episode_reward=-14.97 +/- 23.32
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -15      |
| rollout/            |          |
|    exploration_rate | 0.68     |
| time/               |          |
|    total_timesteps  | 2000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.295    |
|    n_updates        | 474      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -63.5    |
|    exploration_rate | 0.628    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 684      |
|    time_elapsed     | 3        |
|    total_timesteps  | 2328     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.293    |
|    n_updates        | 556      |
----------------------------------
Eval num_timesteps=2500, episode_reward=-10.97 +/- 15.49
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -11      |
| rollout/            |          |
|    exploration_rate | 0.6      |
| time/               |          |
|    total_timesteps  | 2500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.586    |
|    n_updates        | 599      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -55.3    |
|    exploration_rate | 0.565    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 688      |
|    time_elapsed     | 3        |
|    total_timesteps  | 2716     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.291    |
|    n_updates        | 653      |
----------------------------------
Eval num_timesteps=3000, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.52     |
| time/               |          |
|    total_timesteps  | 3000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00135  |
|    n_updates        | 724      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -48.5    |
|    exploration_rate | 0.503    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 684      |
|    time_elapsed     | 4        |
|    total_timesteps  | 3104     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.584    |
|    n_updates        | 750      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -43.2    |
|    exploration_rate | 0.441    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 641      |
|    time_elapsed     | 5        |
|    total_timesteps  | 3492     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.291    |
|    n_updates        | 847      |
----------------------------------
Eval num_timesteps=3500, episode_reward=-6.97 +/- 8.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -6.97    |
| rollout/            |          |
|    exploration_rate | 0.44     |
| time/               |          |
|    total_timesteps  | 3500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0026   |
|    n_updates        | 849      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -39.2    |
|    exploration_rate | 0.379    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 643      |
|    time_elapsed     | 6        |
|    total_timesteps  | 3880     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.869    |
|    n_updates        | 944      |
----------------------------------
Eval num_timesteps=4000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.36     |
| time/               |          |
|    total_timesteps  | 4000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.293    |
|    n_updates        | 974      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -35.7    |
|    exploration_rate | 0.317    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 642      |
|    time_elapsed     | 6        |
|    total_timesteps  | 4268     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00738  |
|    n_updates        | 1041     |
----------------------------------
Eval num_timesteps=4500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.28     |
| time/               |          |
|    total_timesteps  | 4500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.289    |
|    n_updates        | 1099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -32.8    |
|    exploration_rate | 0.255    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 640      |
|    time_elapsed     | 7        |
|    total_timesteps  | 4656     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.3      |
|    n_updates        | 1138     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 5000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.574    |
|    n_updates        | 1224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -30.6    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 52       |
|    fps              | 609      |
|    time_elapsed     | 8        |
|    total_timesteps  | 5044     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.304    |
|    n_updates        | 1235     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -28.5    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 56       |
|    fps              | 615      |
|    time_elapsed     | 8        |
|    total_timesteps  | 5432     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00784  |
|    n_updates        | 1332     |
----------------------------------
Eval num_timesteps=5500, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 5500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.3      |
|    n_updates        | 1349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -26.6    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 60       |
|    fps              | 611      |
|    time_elapsed     | 9        |
|    total_timesteps  | 5820     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.275    |
|    n_updates        | 1429     |
----------------------------------
Eval num_timesteps=6000, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 6000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00316  |
|    n_updates        | 1474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -25      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 64       |
|    fps              | 610      |
|    time_elapsed     | 10       |
|    total_timesteps  | 6208     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000412 |
|    n_updates        | 1526     |
----------------------------------
Eval num_timesteps=6500, episode_reward=-4.97 +/- 4.90
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -4.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 6500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00225  |
|    n_updates        | 1599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -23.6    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 68       |
|    fps              | 611      |
|    time_elapsed     | 10       |
|    total_timesteps  | 6596     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.559    |
|    n_updates        | 1623     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -22.5    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 72       |
|    fps              | 619      |
|    time_elapsed     | 11       |
|    total_timesteps  | 6984     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00133  |
|    n_updates        | 1720     |
----------------------------------
Eval num_timesteps=7000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 7000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00141  |
|    n_updates        | 1724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -21.4    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 76       |
|    fps              | 617      |
|    time_elapsed     | 11       |
|    total_timesteps  | 7372     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.282    |
|    n_updates        | 1817     |
----------------------------------
Eval num_timesteps=7500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 7500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.28     |
|    n_updates        | 1849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -20.3    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 80       |
|    fps              | 615      |
|    time_elapsed     | 12       |
|    total_timesteps  | 7760     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00632  |
|    n_updates        | 1914     |
----------------------------------
Eval num_timesteps=8000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 8000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00126  |
|    n_updates        | 1974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -19.4    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 84       |
|    fps              | 611      |
|    time_elapsed     | 13       |
|    total_timesteps  | 8148     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00103  |
|    n_updates        | 2011     |
----------------------------------
Eval num_timesteps=8500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 8500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0014   |
|    n_updates        | 2099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -18.6    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 88       |
|    fps              | 583      |
|    time_elapsed     | 14       |
|    total_timesteps  | 8536     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00251  |
|    n_updates        | 2108     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -17.8    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 92       |
|    fps              | 590      |
|    time_elapsed     | 15       |
|    total_timesteps  | 8924     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00543  |
|    n_updates        | 2205     |
----------------------------------
Eval num_timesteps=9000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.261    |
|    n_updates        | 2224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -17.1    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 96       |
|    fps              | 589      |
|    time_elapsed     | 15       |
|    total_timesteps  | 9312     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00297  |
|    n_updates        | 2302     |
----------------------------------
Eval num_timesteps=9500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 9500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00288  |
|    n_updates        | 2349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -16.5    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 100      |
|    fps              | 590      |
|    time_elapsed     | 16       |
|    total_timesteps  | 9700     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.265    |
|    n_updates        | 2399     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 10000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00399  |
|    n_updates        | 2474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -13.1    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 104      |
|    fps              | 591      |
|    time_elapsed     | 17       |
|    total_timesteps  | 10088    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 2496     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -6.87    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 108      |
|    fps              | 597      |
|    time_elapsed     | 17       |
|    total_timesteps  | 10476    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.245    |
|    n_updates        | 2593     |
----------------------------------
Eval num_timesteps=10500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00617  |
|    n_updates        | 2599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -6.07    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 112      |
|    fps              | 598      |
|    time_elapsed     | 18       |
|    total_timesteps  | 10864    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.266    |
|    n_updates        | 2690     |
----------------------------------
Eval num_timesteps=11000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 11000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00599  |
|    n_updates        | 2724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -3.27    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 116      |
|    fps              | 597      |
|    time_elapsed     | 18       |
|    total_timesteps  | 11252    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000682 |
|    n_updates        | 2787     |
----------------------------------
Eval num_timesteps=11500, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 11500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000323 |
|    n_updates        | 2849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -2.27    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 120      |
|    fps              | 593      |
|    time_elapsed     | 19       |
|    total_timesteps  | 11640    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.269    |
|    n_updates        | 2884     |
----------------------------------
Eval num_timesteps=12000, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 12000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.266    |
|    n_updates        | 2974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.67    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 124      |
|    fps              | 592      |
|    time_elapsed     | 20       |
|    total_timesteps  | 12028    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00148  |
|    n_updates        | 2981     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 128      |
|    fps              | 596      |
|    time_elapsed     | 20       |
|    total_timesteps  | 12416    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000572 |
|    n_updates        | 3078     |
----------------------------------
Eval num_timesteps=12500, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 12500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00607  |
|    n_updates        | 3099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 132      |
|    fps              | 597      |
|    time_elapsed     | 21       |
|    total_timesteps  | 12804    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.248    |
|    n_updates        | 3175     |
----------------------------------
Eval num_timesteps=13000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 13000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.96e-05 |
|    n_updates        | 3224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 136      |
|    fps              | 597      |
|    time_elapsed     | 22       |
|    total_timesteps  | 13192    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.198    |
|    n_updates        | 3272     |
----------------------------------
Eval num_timesteps=13500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 13500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000647 |
|    n_updates        | 3349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.37    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 140      |
|    fps              | 598      |
|    time_elapsed     | 22       |
|    total_timesteps  | 13580    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00349  |
|    n_updates        | 3369     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.37    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 144      |
|    fps              | 588      |
|    time_elapsed     | 23       |
|    total_timesteps  | 13968    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000373 |
|    n_updates        | 3466     |
----------------------------------
Eval num_timesteps=14000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 14000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000731 |
|    n_updates        | 3474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 148      |
|    fps              | 589      |
|    time_elapsed     | 24       |
|    total_timesteps  | 14356    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00239  |
|    n_updates        | 3563     |
----------------------------------
Eval num_timesteps=14500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 14500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.693    |
|    n_updates        | 3599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.77    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 152      |
|    fps              | 589      |
|    time_elapsed     | 25       |
|    total_timesteps  | 14744    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00833  |
|    n_updates        | 3660     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000411 |
|    n_updates        | 3724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.77    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 156      |
|    fps              | 590      |
|    time_elapsed     | 25       |
|    total_timesteps  | 15132    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.93e-05 |
|    n_updates        | 3757     |
----------------------------------
Eval num_timesteps=15500, episode_reward=-16.97 +/- 10.20
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -17      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 15500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000663 |
|    n_updates        | 3849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.77    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 160      |
|    fps              | 584      |
|    time_elapsed     | 26       |
|    total_timesteps  | 15520    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000547 |
|    n_updates        | 3854     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -1.97    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 164      |
|    fps              | 588      |
|    time_elapsed     | 27       |
|    total_timesteps  | 15908    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00166  |
|    n_updates        | 3951     |
----------------------------------
Eval num_timesteps=16000, episode_reward=-10.97 +/- 12.65
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -11      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 16000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.247    |
|    n_updates        | 3974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -3.37    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 168      |
|    fps              | 585      |
|    time_elapsed     | 27       |
|    total_timesteps  | 16296    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000646 |
|    n_updates        | 4048     |
----------------------------------
Eval num_timesteps=16500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 16500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.205    |
|    n_updates        | 4099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -3.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 172      |
|    fps              | 573      |
|    time_elapsed     | 29       |
|    total_timesteps  | 16684    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000514 |
|    n_updates        | 4145     |
----------------------------------
Eval num_timesteps=17000, episode_reward=-22.97 +/- 7.48
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -23      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 17000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00164  |
|    n_updates        | 4224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -4.17    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 176      |
|    fps              | 575      |
|    time_elapsed     | 29       |
|    total_timesteps  | 17072    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.247    |
|    n_updates        | 4242     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -4.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 180      |
|    fps              | 576      |
|    time_elapsed     | 30       |
|    total_timesteps  | 17460    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00189  |
|    n_updates        | 4339     |
----------------------------------
Eval num_timesteps=17500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 17500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 4349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -4.67    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 184      |
|    fps              | 577      |
|    time_elapsed     | 30       |
|    total_timesteps  | 17848    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00439  |
|    n_updates        | 4436     |
----------------------------------
Eval num_timesteps=18000, episode_reward=-16.97 +/- 8.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -17      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000123 |
|    n_updates        | 4474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -5.17    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 188      |
|    fps              | 578      |
|    time_elapsed     | 31       |
|    total_timesteps  | 18236    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.445    |
|    n_updates        | 4533     |
----------------------------------
Eval num_timesteps=18500, episode_reward=-4.97 +/- 4.90
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -4.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.45e-05 |
|    n_updates        | 4599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -5.57    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 192      |
|    fps              | 565      |
|    time_elapsed     | 32       |
|    total_timesteps  | 18624    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 4630     |
----------------------------------
Eval num_timesteps=19000, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 19000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00352  |
|    n_updates        | 4724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -5.97    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 196      |
|    fps              | 559      |
|    time_elapsed     | 33       |
|    total_timesteps  | 19012    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 4727     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -6.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 200      |
|    fps              | 561      |
|    time_elapsed     | 34       |
|    total_timesteps  | 19400    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00902  |
|    n_updates        | 4824     |
----------------------------------
Eval num_timesteps=19500, episode_reward=-20.97 +/- 14.14
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -21      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 19500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00259  |
|    n_updates        | 4849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -6.87    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 204      |
|    fps              | 561      |
|    time_elapsed     | 35       |
|    total_timesteps  | 19788    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.213    |
|    n_updates        | 4921     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.21     |
|    n_updates        | 4974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -6.87    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 208      |
|    fps              | 542      |
|    time_elapsed     | 37       |
|    total_timesteps  | 20176    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00065  |
|    n_updates        | 5018     |
----------------------------------
Eval num_timesteps=20500, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 20500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00428  |
|    n_updates        | 5099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -6.77    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 212      |
|    fps              | 518      |
|    time_elapsed     | 39       |
|    total_timesteps  | 20564    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000905 |
|    n_updates        | 5115     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -6.97    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 216      |
|    fps              | 522      |
|    time_elapsed     | 40       |
|    total_timesteps  | 20952    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00158  |
|    n_updates        | 5212     |
----------------------------------
Eval num_timesteps=21000, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 21000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00855  |
|    n_updates        | 5224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -6.97    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 220      |
|    fps              | 523      |
|    time_elapsed     | 40       |
|    total_timesteps  | 21340    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000334 |
|    n_updates        | 5309     |
----------------------------------
Eval num_timesteps=21500, episode_reward=-4.97 +/- 4.90
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -4.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 21500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 5349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -6.97    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 224      |
|    fps              | 507      |
|    time_elapsed     | 42       |
|    total_timesteps  | 21728    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0799   |
|    n_updates        | 5406     |
----------------------------------
Eval num_timesteps=22000, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 22000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000454 |
|    n_updates        | 5474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -7.67    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 228      |
|    fps              | 494      |
|    time_elapsed     | 44       |
|    total_timesteps  | 22116    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000584 |
|    n_updates        | 5503     |
----------------------------------
Eval num_timesteps=22500, episode_reward=-4.97 +/- 8.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -4.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 22500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0786   |
|    n_updates        | 5599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -7.77    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 232      |
|    fps              | 495      |
|    time_elapsed     | 45       |
|    total_timesteps  | 22504    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00311  |
|    n_updates        | 5600     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.07    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 236      |
|    fps              | 499      |
|    time_elapsed     | 45       |
|    total_timesteps  | 22892    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000167 |
|    n_updates        | 5697     |
----------------------------------
Eval num_timesteps=23000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 23000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000115 |
|    n_updates        | 5724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.17    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 240      |
|    fps              | 500      |
|    time_elapsed     | 46       |
|    total_timesteps  | 23280    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 5794     |
----------------------------------
Eval num_timesteps=23500, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 23500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00105  |
|    n_updates        | 5849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.27    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 244      |
|    fps              | 492      |
|    time_elapsed     | 48       |
|    total_timesteps  | 23668    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.58e-05 |
|    n_updates        | 5891     |
----------------------------------
Eval num_timesteps=24000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 24000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000751 |
|    n_updates        | 5974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.67    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 248      |
|    fps              | 487      |
|    time_elapsed     | 49       |
|    total_timesteps  | 24056    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00214  |
|    n_updates        | 5988     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.37    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 252      |
|    fps              | 490      |
|    time_elapsed     | 49       |
|    total_timesteps  | 24444    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000143 |
|    n_updates        | 6085     |
----------------------------------
Eval num_timesteps=24500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 24500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00476  |
|    n_updates        | 6099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 256      |
|    fps              | 478      |
|    time_elapsed     | 51       |
|    total_timesteps  | 24832    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000596 |
|    n_updates        | 6182     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-20.97 +/- 6.32
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -21      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0924   |
|    n_updates        | 6224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.17    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 260      |
|    fps              | 479      |
|    time_elapsed     | 52       |
|    total_timesteps  | 25220    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0053   |
|    n_updates        | 6279     |
----------------------------------
Eval num_timesteps=25500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 25500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000137 |
|    n_updates        | 6349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 264      |
|    fps              | 479      |
|    time_elapsed     | 53       |
|    total_timesteps  | 25608    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000496 |
|    n_updates        | 6376     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.87    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 268      |
|    fps              | 482      |
|    time_elapsed     | 53       |
|    total_timesteps  | 25996    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 6473     |
----------------------------------
Eval num_timesteps=26000, episode_reward=-8.97 +/- 11.66
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -8.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 26000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000247 |
|    n_updates        | 6474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.67    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 272      |
|    fps              | 483      |
|    time_elapsed     | 54       |
|    total_timesteps  | 26384    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.55e-05 |
|    n_updates        | 6570     |
----------------------------------
Eval num_timesteps=26500, episode_reward=-4.97 +/- 8.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -4.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 26500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000601 |
|    n_updates        | 6599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.07    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 276      |
|    fps              | 484      |
|    time_elapsed     | 55       |
|    total_timesteps  | 26772    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000214 |
|    n_updates        | 6667     |
----------------------------------
Eval num_timesteps=27000, episode_reward=-8.97 +/- 7.48
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -8.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000387 |
|    n_updates        | 6724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.07    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 280      |
|    fps              | 485      |
|    time_elapsed     | 55       |
|    total_timesteps  | 27160    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0627   |
|    n_updates        | 6764     |
----------------------------------
Eval num_timesteps=27500, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 27500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000665 |
|    n_updates        | 6849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.07    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 284      |
|    fps              | 487      |
|    time_elapsed     | 56       |
|    total_timesteps  | 27548    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000229 |
|    n_updates        | 6861     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -7.77    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 288      |
|    fps              | 488      |
|    time_elapsed     | 57       |
|    total_timesteps  | 27936    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0868   |
|    n_updates        | 6958     |
----------------------------------
Eval num_timesteps=28000, episode_reward=-6.97 +/- 4.90
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -6.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 28000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000317 |
|    n_updates        | 6974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -7.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 292      |
|    fps              | 488      |
|    time_elapsed     | 57       |
|    total_timesteps  | 28324    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000117 |
|    n_updates        | 7055     |
----------------------------------
Eval num_timesteps=28500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 28500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00214  |
|    n_updates        | 7099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.57    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 296      |
|    fps              | 475      |
|    time_elapsed     | 60       |
|    total_timesteps  | 28712    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000255 |
|    n_updates        | 7152     |
----------------------------------
Eval num_timesteps=29000, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 29000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 9.08e-05 |
|    n_updates        | 7224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.07    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 300      |
|    fps              | 475      |
|    time_elapsed     | 61       |
|    total_timesteps  | 29100    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00023  |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.07    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 304      |
|    fps              | 478      |
|    time_elapsed     | 61       |
|    total_timesteps  | 29488    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000544 |
|    n_updates        | 7346     |
----------------------------------
Eval num_timesteps=29500, episode_reward=-6.97 +/- 4.90
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -6.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 29500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00486  |
|    n_updates        | 7349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.07    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 308      |
|    fps              | 475      |
|    time_elapsed     | 62       |
|    total_timesteps  | 29876    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.3e-05  |
|    n_updates        | 7443     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0277   |
|    n_updates        | 7474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.37    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 312      |
|    fps              | 469      |
|    time_elapsed     | 64       |
|    total_timesteps  | 30264    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00356  |
|    n_updates        | 7540     |
----------------------------------
Eval num_timesteps=30500, episode_reward=-36.97 +/- 19.60
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -37      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 30500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000173 |
|    n_updates        | 7599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.37    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 316      |
|    fps              | 470      |
|    time_elapsed     | 65       |
|    total_timesteps  | 30652    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 9.51e-05 |
|    n_updates        | 7637     |
----------------------------------
Eval num_timesteps=31000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 31000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 7.44e-05 |
|    n_updates        | 7724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.77    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 320      |
|    fps              | 471      |
|    time_elapsed     | 65       |
|    total_timesteps  | 31040    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 9.14e-05 |
|    n_updates        | 7734     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.67    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 324      |
|    fps              | 474      |
|    time_elapsed     | 66       |
|    total_timesteps  | 31428    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 7.72e-05 |
|    n_updates        | 7831     |
----------------------------------
Eval num_timesteps=31500, episode_reward=-2.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -2.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 31500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000271 |
|    n_updates        | 7849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.27    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 328      |
|    fps              | 475      |
|    time_elapsed     | 66       |
|    total_timesteps  | 31816    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000389 |
|    n_updates        | 7928     |
----------------------------------
Eval num_timesteps=32000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 32000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.83e-05 |
|    n_updates        | 7974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.37    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 332      |
|    fps              | 474      |
|    time_elapsed     | 67       |
|    total_timesteps  | 32204    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 7.03e-05 |
|    n_updates        | 8025     |
----------------------------------
Eval num_timesteps=32500, episode_reward=-10.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -11      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 32500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000246 |
|    n_updates        | 8099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -8.67    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 336      |
|    fps              | 458      |
|    time_elapsed     | 71       |
|    total_timesteps  | 32592    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.79e-05 |
|    n_updates        | 8122     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.17    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 340      |
|    fps              | 444      |
|    time_elapsed     | 74       |
|    total_timesteps  | 32980    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00152  |
|    n_updates        | 8219     |
----------------------------------
Eval num_timesteps=33000, episode_reward=-8.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -8.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 33000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.21e-05 |
|    n_updates        | 8224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.67    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 344      |
|    fps              | 446      |
|    time_elapsed     | 74       |
|    total_timesteps  | 33368    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00038  |
|    n_updates        | 8316     |
----------------------------------
Eval num_timesteps=33500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 33500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000337 |
|    n_updates        | 8349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 348      |
|    fps              | 446      |
|    time_elapsed     | 75       |
|    total_timesteps  | 33756    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00432  |
|    n_updates        | 8413     |
----------------------------------
Eval num_timesteps=34000, episode_reward=-22.97 +/- 11.66
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -23      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 34000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.267    |
|    n_updates        | 8474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.57    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 352      |
|    fps              | 440      |
|    time_elapsed     | 77       |
|    total_timesteps  | 34144    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 8510     |
----------------------------------
Eval num_timesteps=34500, episode_reward=-8.97 +/- 11.66
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -8.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 34500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00023  |
|    n_updates        | 8599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.97    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 356      |
|    fps              | 437      |
|    time_elapsed     | 78       |
|    total_timesteps  | 34532    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000493 |
|    n_updates        | 8607     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.47    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 360      |
|    fps              | 440      |
|    time_elapsed     | 79       |
|    total_timesteps  | 34920    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 8704     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-8.97 +/- 16.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -8.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.37e-05 |
|    n_updates        | 8724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.57    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 364      |
|    fps              | 428      |
|    time_elapsed     | 82       |
|    total_timesteps  | 35308    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 9.76e-05 |
|    n_updates        | 8801     |
----------------------------------
Eval num_timesteps=35500, episode_reward=-6.97 +/- 4.90
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -6.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 35500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000205 |
|    n_updates        | 8849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -9.07    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 368      |
|    fps              | 429      |
|    time_elapsed     | 83       |
|    total_timesteps  | 35696    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000286 |
|    n_updates        | 8898     |
----------------------------------
Eval num_timesteps=36000, episode_reward=-8.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -8.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00303  |
|    n_updates        | 8974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -10.1    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 372      |
|    fps              | 431      |
|    time_elapsed     | 83       |
|    total_timesteps  | 36084    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000545 |
|    n_updates        | 8995     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -10.3    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 376      |
|    fps              | 433      |
|    time_elapsed     | 84       |
|    total_timesteps  | 36472    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000166 |
|    n_updates        | 9092     |
----------------------------------
Eval num_timesteps=36500, episode_reward=-12.97 +/- 4.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -13      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 36500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.04e-05 |
|    n_updates        | 9099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -10.5    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 380      |
|    fps              | 434      |
|    time_elapsed     | 84       |
|    total_timesteps  | 36860    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 7.25e-05 |
|    n_updates        | 9189     |
----------------------------------
Eval num_timesteps=37000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 37000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000232 |
|    n_updates        | 9224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -10.8    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 384      |
|    fps              | 420      |
|    time_elapsed     | 88       |
|    total_timesteps  | 37248    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.78e-05 |
|    n_updates        | 9286     |
----------------------------------
Eval num_timesteps=37500, episode_reward=-14.97 +/- 14.97
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -15      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 37500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000337 |
|    n_updates        | 9349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.3    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 388      |
|    fps              | 421      |
|    time_elapsed     | 89       |
|    total_timesteps  | 37636    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.81e-05 |
|    n_updates        | 9383     |
----------------------------------
Eval num_timesteps=38000, episode_reward=-8.97 +/- 9.80
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -8.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 38000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00141  |
|    n_updates        | 9474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.7    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 392      |
|    fps              | 414      |
|    time_elapsed     | 91       |
|    total_timesteps  | 38024    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.01e-05 |
|    n_updates        | 9480     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -10.9    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 396      |
|    fps              | 402      |
|    time_elapsed     | 95       |
|    total_timesteps  | 38412    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000369 |
|    n_updates        | 9577     |
----------------------------------
Eval num_timesteps=38500, episode_reward=-22.97 +/- 9.80
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -23      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 38500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00018  |
|    n_updates        | 9599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.4    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 400      |
|    fps              | 403      |
|    time_elapsed     | 96       |
|    total_timesteps  | 38800    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 9.94e-05 |
|    n_updates        | 9674     |
----------------------------------
Eval num_timesteps=39000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 39000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000422 |
|    n_updates        | 9724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.2    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 404      |
|    fps              | 405      |
|    time_elapsed     | 96       |
|    total_timesteps  | 39188    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000301 |
|    n_updates        | 9771     |
----------------------------------
Eval num_timesteps=39500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 39500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000205 |
|    n_updates        | 9849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.2    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 408      |
|    fps              | 404      |
|    time_elapsed     | 97       |
|    total_timesteps  | 39576    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.05e-05 |
|    n_updates        | 9868     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.2    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 412      |
|    fps              | 405      |
|    time_elapsed     | 98       |
|    total_timesteps  | 39964    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000158 |
|    n_updates        | 9965     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 9.67e-05 |
|    n_updates        | 9974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.2    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 416      |
|    fps              | 407      |
|    time_elapsed     | 99       |
|    total_timesteps  | 40352    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000725 |
|    n_updates        | 10062    |
----------------------------------
Eval num_timesteps=40500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000605 |
|    n_updates        | 10099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.4    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 420      |
|    fps              | 408      |
|    time_elapsed     | 99       |
|    total_timesteps  | 40740    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00116  |
|    n_updates        | 10159    |
----------------------------------
Eval num_timesteps=41000, episode_reward=-4.97 +/- 4.90
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -4.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.06e-05 |
|    n_updates        | 10224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.7    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 424      |
|    fps              | 409      |
|    time_elapsed     | 100      |
|    total_timesteps  | 41128    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000188 |
|    n_updates        | 10256    |
----------------------------------
Eval num_timesteps=41500, episode_reward=-12.97 +/- 19.39
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -13      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000196 |
|    n_updates        | 10349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -12      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 428      |
|    fps              | 411      |
|    time_elapsed     | 100      |
|    total_timesteps  | 41516    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 10353    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -12.7    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 432      |
|    fps              | 412      |
|    time_elapsed     | 101      |
|    total_timesteps  | 41904    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00131  |
|    n_updates        | 10450    |
----------------------------------
Eval num_timesteps=42000, episode_reward=-18.97 +/- 7.48
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -19      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000897 |
|    n_updates        | 10474    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -12.4    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 436      |
|    fps              | 409      |
|    time_elapsed     | 103      |
|    total_timesteps  | 42292    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0759   |
|    n_updates        | 10547    |
----------------------------------
Eval num_timesteps=42500, episode_reward=-4.97 +/- 8.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -4.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000108 |
|    n_updates        | 10599    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -12.1    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 440      |
|    fps              | 399      |
|    time_elapsed     | 106      |
|    total_timesteps  | 42680    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.059    |
|    n_updates        | 10644    |
----------------------------------
Eval num_timesteps=43000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000103 |
|    n_updates        | 10724    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -11.8    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 444      |
|    fps              | 398      |
|    time_elapsed     | 108      |
|    total_timesteps  | 43068    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000816 |
|    n_updates        | 10741    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -12.4    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 448      |
|    fps              | 395      |
|    time_elapsed     | 109      |
|    total_timesteps  | 43456    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0016   |
|    n_updates        | 10838    |
----------------------------------
Eval num_timesteps=43500, episode_reward=-6.97 +/- 8.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -6.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.001    |
|    n_updates        | 10849    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -12.8    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 452      |
|    fps              | 395      |
|    time_elapsed     | 110      |
|    total_timesteps  | 43844    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000374 |
|    n_updates        | 10935    |
----------------------------------
Eval num_timesteps=44000, episode_reward=-10.97 +/- 10.95
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -11      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00149  |
|    n_updates        | 10974    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -13.5    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 456      |
|    fps              | 396      |
|    time_elapsed     | 111      |
|    total_timesteps  | 44232    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0934   |
|    n_updates        | 11032    |
----------------------------------
Eval num_timesteps=44500, episode_reward=-24.97 +/- 10.20
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -25      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00192  |
|    n_updates        | 11099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -14.1    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 460      |
|    fps              | 398      |
|    time_elapsed     | 112      |
|    total_timesteps  | 44620    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000658 |
|    n_updates        | 11129    |
----------------------------------
Eval num_timesteps=45000, episode_reward=-20.97 +/- 6.32
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -21      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000809 |
|    n_updates        | 11224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -14.6    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 464      |
|    fps              | 394      |
|    time_elapsed     | 114      |
|    total_timesteps  | 45008    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.01e-05 |
|    n_updates        | 11226    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -14.7    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 468      |
|    fps              | 393      |
|    time_elapsed     | 115      |
|    total_timesteps  | 45396    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000202 |
|    n_updates        | 11323    |
----------------------------------
Eval num_timesteps=45500, episode_reward=-12.97 +/- 7.48
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -13      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.11e-05 |
|    n_updates        | 11349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -14      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 472      |
|    fps              | 394      |
|    time_elapsed     | 116      |
|    total_timesteps  | 45784    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000123 |
|    n_updates        | 11420    |
----------------------------------
Eval num_timesteps=46000, episode_reward=-24.97 +/- 12.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -25      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000535 |
|    n_updates        | 11474    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -14.7    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 476      |
|    fps              | 392      |
|    time_elapsed     | 117      |
|    total_timesteps  | 46172    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000933 |
|    n_updates        | 11517    |
----------------------------------
Eval num_timesteps=46500, episode_reward=-26.97 +/- 16.25
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -27      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000216 |
|    n_updates        | 11599    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -14.8    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 480      |
|    fps              | 393      |
|    time_elapsed     | 118      |
|    total_timesteps  | 46560    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000782 |
|    n_updates        | 11614    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -15.1    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 484      |
|    fps              | 394      |
|    time_elapsed     | 118      |
|    total_timesteps  | 46948    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000271 |
|    n_updates        | 11711    |
----------------------------------
Eval num_timesteps=47000, episode_reward=-26.97 +/- 20.59
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -27      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00773  |
|    n_updates        | 11724    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -15.5    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 488      |
|    fps              | 395      |
|    time_elapsed     | 119      |
|    total_timesteps  | 47336    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000323 |
|    n_updates        | 11808    |
----------------------------------
Eval num_timesteps=47500, episode_reward=-10.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -11      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.256    |
|    n_updates        | 11849    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -15.8    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 492      |
|    fps              | 396      |
|    time_elapsed     | 120      |
|    total_timesteps  | 47724    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000187 |
|    n_updates        | 11905    |
----------------------------------
Eval num_timesteps=48000, episode_reward=-26.97 +/- 10.20
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -27      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00929  |
|    n_updates        | 11974    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -15.8    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 496      |
|    fps              | 398      |
|    time_elapsed     | 120      |
|    total_timesteps  | 48112    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 12002    |
----------------------------------
Eval num_timesteps=48500, episode_reward=-8.97 +/- 7.48
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -8.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.001    |
|    n_updates        | 12099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -15.5    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 500      |
|    fps              | 399      |
|    time_elapsed     | 121      |
|    total_timesteps  | 48500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -15.5    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 504      |
|    fps              | 400      |
|    time_elapsed     | 122      |
|    total_timesteps  | 48888    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000246 |
|    n_updates        | 12196    |
----------------------------------
Eval num_timesteps=49000, episode_reward=-10.97 +/- 8.94
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -11      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000283 |
|    n_updates        | 12224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -16.1    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 508      |
|    fps              | 400      |
|    time_elapsed     | 123      |
|    total_timesteps  | 49276    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000919 |
|    n_updates        | 12293    |
----------------------------------
Eval num_timesteps=49500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000267 |
|    n_updates        | 12349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -16.3    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 512      |
|    fps              | 401      |
|    time_elapsed     | 123      |
|    total_timesteps  | 49664    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 9.69e-05 |
|    n_updates        | 12390    |
----------------------------------
Eval num_timesteps=50000, episode_reward=-18.97 +/- 7.48
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -19      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.075    |
|    n_updates        | 12474    |
----------------------------------
 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50,000/50,000  [ 0:02:04 < 0:00:00 , 395 it/s ]
515
515
[[[1. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 ...

 [[0. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]]
Agent: [0 0]
Target: [9 9]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 3
location: [1 0]
Action: 3
location: [2 0]
Action: 1
location: [1 0]
Action: 1
location: [0 0]
Action: 0
location: [0 1]
Action: 2
location: [0 0]
Action: 2
location: [0 0]
Action: 3
location: [1 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 2
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 2
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 3
location: [1 0]
Action: 3
location: [2 0]
Action: 1
location: [1 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 3
location: [1 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 3
location: [1 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 2
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 3
location: [1 0]
Action: 0
location: [1 1]
Action: 0
location: [1 2]
Action: 1
location: [0 2]
Action: 3
location: [1 2]
Action: 1
location: [0 2]
Action: 2
location: [0 1]
Action: 2
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 3
location: [1 0]
Action: 1
location: [0 0]
Action: 0
location: [0 1]
Action: 2
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 1
location: [0 0]
Action: 0
location: [0 1]
Action: 0
location: [0 2]
Action: 3
location: [1 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 3
location: [1 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 3
location: [1 2]
Action: 3
location: [2 2]
Action: 1
location: [1 2]
Action: 1
location: [0 2]
Action: 3
location: [1 2]
Action: 1
location: [0 2]
Action: 1
location: [0 2]
Action: 2
location: [0 1]
Action: 2
location: [0 0]
Completed in 96 steps with score of -0.01
(0, 0)
(9, 9)
['agent     ', 'explored  ', 'explored  ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['explored  ', 'explored  ', 'explored  ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['explored  ', '          ', 'explored  ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'target    ']
Title: dqn path
Save path: ./graphs/dqnpath.pngdqn path
Execution time: 147.09105587005615 seconds
