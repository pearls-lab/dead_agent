pygame 2.1.3 (SDL 2.0.22, Python 3.11.1)
Hello from the pygame community. https://www.pygame.org/contribute.html
Using cuda device
Wrapping the env in a DummyVecEnv.
Testing algo: dqn
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -68.5    |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 779      |
|    time_elapsed     | 0        |
|    total_timesteps  | 388      |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.887    |
|    n_updates        | 71       |
----------------------------------
Eval num_timesteps=500, episode_reward=-66.97 +/- 48.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -67      |
| rollout/            |          |
|    exploration_rate | 0.92     |
| time/               |          |
|    total_timesteps  | 500      |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.592    |
|    n_updates        | 99       |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | -52.8    |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 922      |
|    time_elapsed     | 0        |
|    total_timesteps  | 773      |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.295    |
|    n_updates        | 168      |
----------------------------------
Eval num_timesteps=1000, episode_reward=-61.91 +/- 53.70
Episode length: 91.00 +/- 12.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91       |
|    mean_reward      | -61.9    |
| rollout/            |          |
|    exploration_rate | 0.84     |
| time/               |          |
|    total_timesteps  | 1000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.33     |
|    n_updates        | 224      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.2     |
|    ep_rew_mean      | -40.1    |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 969      |
|    time_elapsed     | 1        |
|    total_timesteps  | 1118     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.295    |
|    n_updates        | 254      |
----------------------------------
Eval num_timesteps=1500, episode_reward=-14.79 +/- 15.87
Episode length: 79.40 +/- 22.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.4     |
|    mean_reward      | -14.8    |
| rollout/            |          |
|    exploration_rate | 0.76     |
| time/               |          |
|    total_timesteps  | 1500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000187 |
|    n_updates        | 349      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.1     |
|    ep_rew_mean      | -35      |
|    exploration_rate | 0.759    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 997      |
|    time_elapsed     | 1        |
|    total_timesteps  | 1506     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000309 |
|    n_updates        | 351      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | -33.7    |
|    exploration_rate | 0.703    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1050     |
|    time_elapsed     | 1        |
|    total_timesteps  | 1856     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.591    |
|    n_updates        | 438      |
----------------------------------
Eval num_timesteps=2000, episode_reward=-21.77 +/- 23.18
Episode length: 77.40 +/- 18.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.4     |
|    mean_reward      | -21.8    |
| rollout/            |          |
|    exploration_rate | 0.68     |
| time/               |          |
|    total_timesteps  | 2000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.294    |
|    n_updates        | 474      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.1     |
|    ep_rew_mean      | -29.6    |
|    exploration_rate | 0.658    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1035     |
|    time_elapsed     | 2        |
|    total_timesteps  | 2139     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.59     |
|    n_updates        | 509      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | -28.9    |
|    exploration_rate | 0.606    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1063     |
|    time_elapsed     | 2        |
|    total_timesteps  | 2464     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.736    |
|    n_updates        | 590      |
----------------------------------
Eval num_timesteps=2500, episode_reward=-22.65 +/- 31.39
Episode length: 65.00 +/- 20.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65       |
|    mean_reward      | -22.6    |
| rollout/            |          |
|    exploration_rate | 0.6      |
| time/               |          |
|    total_timesteps  | 2500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 599      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.3     |
|    ep_rew_mean      | -27.9    |
|    exploration_rate | 0.558    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1051     |
|    time_elapsed     | 2        |
|    total_timesteps  | 2763     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.296    |
|    n_updates        | 665      |
----------------------------------
Eval num_timesteps=3000, episode_reward=-4.97 +/- 8.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -4.97    |
| rollout/            |          |
|    exploration_rate | 0.52     |
| time/               |          |
|    total_timesteps  | 3000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000231 |
|    n_updates        | 724      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.4     |
|    ep_rew_mean      | -24.7    |
|    exploration_rate | 0.514    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1016     |
|    time_elapsed     | 2        |
|    total_timesteps  | 3040     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.296    |
|    n_updates        | 734      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | -23.1    |
|    exploration_rate | 0.471    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1026     |
|    time_elapsed     | 3        |
|    total_timesteps  | 3306     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.297    |
|    n_updates        | 801      |
----------------------------------
Eval num_timesteps=3500, episode_reward=-0.89 +/- 3.18
Episode length: 89.40 +/- 9.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.4     |
|    mean_reward      | -0.894   |
| rollout/            |          |
|    exploration_rate | 0.44     |
| time/               |          |
|    total_timesteps  | 3500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.885    |
|    n_updates        | 849      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | -20.8    |
|    exploration_rate | 0.425    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 999      |
|    time_elapsed     | 3        |
|    total_timesteps  | 3593     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000385 |
|    n_updates        | 873      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | -19.2    |
|    exploration_rate | 0.363    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 1012     |
|    time_elapsed     | 3        |
|    total_timesteps  | 3981     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.588    |
|    n_updates        | 970      |
----------------------------------
Eval num_timesteps=4000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.36     |
| time/               |          |
|    total_timesteps  | 4000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.587    |
|    n_updates        | 974      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | -17.7    |
|    exploration_rate | 0.314    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 984      |
|    time_elapsed     | 4        |
|    total_timesteps  | 4285     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00236  |
|    n_updates        | 1046     |
----------------------------------
Eval num_timesteps=4500, episode_reward=0.04 +/- 2.02
Episode length: 96.00 +/- 2.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 0.04     |
| rollout/            |          |
|    exploration_rate | 0.28     |
| time/               |          |
|    total_timesteps  | 4500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.289    |
|    n_updates        | 1099     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | -16.4    |
|    exploration_rate | 0.253    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 957      |
|    time_elapsed     | 4        |
|    total_timesteps  | 4666     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.74     |
|    n_updates        | 1141     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 5000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.295    |
|    n_updates        | 1224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | -15.3    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 60       |
|    fps              | 932      |
|    time_elapsed     | 5        |
|    total_timesteps  | 5054     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.294    |
|    n_updates        | 1238     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.4     |
|    ep_rew_mean      | -14.4    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 64       |
|    fps              | 939      |
|    time_elapsed     | 5        |
|    total_timesteps  | 5404     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.296    |
|    n_updates        | 1325     |
----------------------------------
Eval num_timesteps=5500, episode_reward=0.07 +/- 2.09
Episode length: 92.60 +/- 8.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.6     |
|    mean_reward      | 0.074    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 5500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00269  |
|    n_updates        | 1349     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | -13.6    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 68       |
|    fps              | 921      |
|    time_elapsed     | 6        |
|    total_timesteps  | 5792     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000352 |
|    n_updates        | 1422     |
----------------------------------
Eval num_timesteps=6000, episode_reward=0.04 +/- 2.01
Episode length: 96.40 +/- 1.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.4     |
|    mean_reward      | 0.036    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 6000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.293    |
|    n_updates        | 1474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | -12.8    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 72       |
|    fps              | 905      |
|    time_elapsed     | 6        |
|    total_timesteps  | 6172     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000543 |
|    n_updates        | 1517     |
----------------------------------
Eval num_timesteps=6500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 6500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000201 |
|    n_updates        | 1599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.3     |
|    ep_rew_mean      | -12.2    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 76       |
|    fps              | 891      |
|    time_elapsed     | 7        |
|    total_timesteps  | 6560     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000385 |
|    n_updates        | 1614     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | -11.6    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 80       |
|    fps              | 898      |
|    time_elapsed     | 7        |
|    total_timesteps  | 6948     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000152 |
|    n_updates        | 1711     |
----------------------------------
Eval num_timesteps=7000, episode_reward=1.09 +/- 2.53
Episode length: 90.60 +/- 12.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.6     |
|    mean_reward      | 1.09     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 7000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000655 |
|    n_updates        | 1724     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | -11.1    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 84       |
|    fps              | 888      |
|    time_elapsed     | 8        |
|    total_timesteps  | 7336     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000557 |
|    n_updates        | 1808     |
----------------------------------
Eval num_timesteps=7500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 7500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.297    |
|    n_updates        | 1849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.8     |
|    ep_rew_mean      | -10.6    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 88       |
|    fps              | 876      |
|    time_elapsed     | 8        |
|    total_timesteps  | 7724     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000569 |
|    n_updates        | 1905     |
----------------------------------
Eval num_timesteps=8000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 8000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.279    |
|    n_updates        | 1974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.2     |
|    ep_rew_mean      | -10.2    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 92       |
|    fps              | 866      |
|    time_elapsed     | 9        |
|    total_timesteps  | 8112     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000346 |
|    n_updates        | 2002     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.3     |
|    ep_rew_mean      | -9.74    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 96       |
|    fps              | 873      |
|    time_elapsed     | 9        |
|    total_timesteps  | 8477     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00242  |
|    n_updates        | 2094     |
----------------------------------
Eval num_timesteps=8500, episode_reward=0.14 +/- 2.23
Episode length: 85.60 +/- 22.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.6     |
|    mean_reward      | 0.144    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 8500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00251  |
|    n_updates        | 2099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.6     |
|    ep_rew_mean      | -9.23    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 100      |
|    fps              | 864      |
|    time_elapsed     | 10       |
|    total_timesteps  | 8759     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0019   |
|    n_updates        | 2164     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | -6.3     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 104      |
|    fps              | 866      |
|    time_elapsed     | 10       |
|    total_timesteps  | 8879     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000878 |
|    n_updates        | 2194     |
----------------------------------
Eval num_timesteps=9000, episode_reward=2.35 +/- 2.71
Episode length: 65.20 +/- 29.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.2     |
|    mean_reward      | 2.35     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.003    |
|    n_updates        | 2224     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | -4.63    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 108      |
|    fps              | 858      |
|    time_elapsed     | 10       |
|    total_timesteps  | 9034     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000578 |
|    n_updates        | 2233     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | -3.97    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 112      |
|    fps              | 862      |
|    time_elapsed     | 10       |
|    total_timesteps  | 9282     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000468 |
|    n_updates        | 2295     |
----------------------------------
Eval num_timesteps=9500, episode_reward=3.24 +/- 2.11
Episode length: 76.20 +/- 21.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.2     |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 9500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00193  |
|    n_updates        | 2349     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | -3.16    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 116      |
|    fps              | 856      |
|    time_elapsed     | 11       |
|    total_timesteps  | 9627     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00188  |
|    n_updates        | 2381     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | -1.9     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 120      |
|    fps              | 859      |
|    time_elapsed     | 11       |
|    total_timesteps  | 9807     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 2426     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | -1.33    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 124      |
|    fps              | 862      |
|    time_elapsed     | 11       |
|    total_timesteps  | 9937     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 2459     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.56 +/- 7.75
Episode length: 56.20 +/- 21.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.2     |
|    mean_reward      | -0.562   |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 10000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00127  |
|    n_updates        | 2474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | -0.161   |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 128      |
|    fps              | 857      |
|    time_elapsed     | 11       |
|    total_timesteps  | 10073    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.286    |
|    n_updates        | 2493     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 0.801    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 132      |
|    fps              | 860      |
|    time_elapsed     | 11       |
|    total_timesteps  | 10251    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0011   |
|    n_updates        | 2537     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 0.967    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 136      |
|    fps              | 862      |
|    time_elapsed     | 12       |
|    total_timesteps  | 10368    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.293    |
|    n_updates        | 2566     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.5     |
|    ep_rew_mean      | 1.48     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 140      |
|    fps              | 863      |
|    time_elapsed     | 12       |
|    total_timesteps  | 10460    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000354 |
|    n_updates        | 2589     |
----------------------------------
Eval num_timesteps=10500, episode_reward=4.61 +/- 0.10
Episode length: 38.80 +/- 9.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.8     |
|    mean_reward      | 4.61     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.541    |
|    n_updates        | 2599     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 1.6      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 144      |
|    fps              | 860      |
|    time_elapsed     | 12       |
|    total_timesteps  | 10592    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.296    |
|    n_updates        | 2622     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.8     |
|    ep_rew_mean      | 1.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 148      |
|    fps              | 862      |
|    time_elapsed     | 12       |
|    total_timesteps  | 10756    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000513 |
|    n_updates        | 2663     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.7     |
|    ep_rew_mean      | 1.99     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 152      |
|    fps              | 863      |
|    time_elapsed     | 12       |
|    total_timesteps  | 10856    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00208  |
|    n_updates        | 2688     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.9     |
|    ep_rew_mean      | 2.12     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 156      |
|    fps              | 865      |
|    time_elapsed     | 12       |
|    total_timesteps  | 10954    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00136  |
|    n_updates        | 2713     |
----------------------------------
Eval num_timesteps=11000, episode_reward=4.78 +/- 0.03
Episode length: 22.20 +/- 2.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22.2     |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 11000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.403    |
|    n_updates        | 2724     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.9     |
|    ep_rew_mean      | 2.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 160      |
|    fps              | 863      |
|    time_elapsed     | 12       |
|    total_timesteps  | 11046    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000922 |
|    n_updates        | 2736     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.4     |
|    ep_rew_mean      | 2.53     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 164      |
|    fps              | 864      |
|    time_elapsed     | 12       |
|    total_timesteps  | 11143    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 2760     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.4     |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 168      |
|    fps              | 865      |
|    time_elapsed     | 12       |
|    total_timesteps  | 11236    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00222  |
|    n_updates        | 2783     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | 2.93     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 172      |
|    fps              | 867      |
|    time_elapsed     | 13       |
|    total_timesteps  | 11329    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.263    |
|    n_updates        | 2807     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.8     |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 176      |
|    fps              | 868      |
|    time_elapsed     | 13       |
|    total_timesteps  | 11435    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00424  |
|    n_updates        | 2833     |
----------------------------------
Eval num_timesteps=11500, episode_reward=4.75 +/- 0.03
Episode length: 24.80 +/- 2.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 24.8     |
|    mean_reward      | 4.75     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 11500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00127  |
|    n_updates        | 2849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.8     |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 180      |
|    fps              | 867      |
|    time_elapsed     | 13       |
|    total_timesteps  | 11527    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.411    |
|    n_updates        | 2856     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.7     |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 184      |
|    fps              | 868      |
|    time_elapsed     | 13       |
|    total_timesteps  | 11605    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.288    |
|    n_updates        | 2876     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.7     |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 188      |
|    fps              | 869      |
|    time_elapsed     | 13       |
|    total_timesteps  | 11690    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000331 |
|    n_updates        | 2897     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.6     |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 192      |
|    fps              | 870      |
|    time_elapsed     | 13       |
|    total_timesteps  | 11769    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000893 |
|    n_updates        | 2917     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.9     |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 196      |
|    fps              | 871      |
|    time_elapsed     | 13       |
|    total_timesteps  | 11864    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 2940     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | 4.28     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 200      |
|    fps              | 872      |
|    time_elapsed     | 13       |
|    total_timesteps  | 11944    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00406  |
|    n_updates        | 2960     |
----------------------------------
Eval num_timesteps=12000, episode_reward=4.79 +/- 0.03
Episode length: 21.20 +/- 2.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.2     |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 12000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.194    |
|    n_updates        | 2974     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.6     |
|    ep_rew_mean      | 4.28     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 204      |
|    fps              | 870      |
|    time_elapsed     | 13       |
|    total_timesteps  | 12039    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00189  |
|    n_updates        | 2984     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.2     |
|    ep_rew_mean      | 4.29     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 208      |
|    fps              | 872      |
|    time_elapsed     | 13       |
|    total_timesteps  | 12152    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 3012     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.7     |
|    ep_rew_mean      | 4.4      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 212      |
|    fps              | 873      |
|    time_elapsed     | 14       |
|    total_timesteps  | 12251    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00156  |
|    n_updates        | 3037     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.1     |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 216      |
|    fps              | 874      |
|    time_elapsed     | 14       |
|    total_timesteps  | 12333    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0184   |
|    n_updates        | 3058     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.3     |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 220      |
|    fps              | 875      |
|    time_elapsed     | 14       |
|    total_timesteps  | 12433    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.286    |
|    n_updates        | 3083     |
----------------------------------
Eval num_timesteps=12500, episode_reward=4.79 +/- 0.02
Episode length: 21.20 +/- 1.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.2     |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 12500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00277  |
|    n_updates        | 3099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 224      |
|    fps              | 874      |
|    time_elapsed     | 14       |
|    total_timesteps  | 12522    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00436  |
|    n_updates        | 3105     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 228      |
|    fps              | 875      |
|    time_elapsed     | 14       |
|    total_timesteps  | 12612    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 3127     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 232      |
|    fps              | 877      |
|    time_elapsed     | 14       |
|    total_timesteps  | 12723    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0658   |
|    n_updates        | 3155     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 236      |
|    fps              | 879      |
|    time_elapsed     | 14       |
|    total_timesteps  | 12882    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0115   |
|    n_updates        | 3195     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 240      |
|    fps              | 880      |
|    time_elapsed     | 14       |
|    total_timesteps  | 12977    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.394    |
|    n_updates        | 3219     |
----------------------------------
Eval num_timesteps=13000, episode_reward=4.80 +/- 0.02
Episode length: 20.40 +/- 2.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.4     |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 13000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 3224     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 244      |
|    fps              | 877      |
|    time_elapsed     | 14       |
|    total_timesteps  | 13063    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.402    |
|    n_updates        | 3240     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 248      |
|    fps              | 879      |
|    time_elapsed     | 14       |
|    total_timesteps  | 13154    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0869   |
|    n_updates        | 3263     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 252      |
|    fps              | 880      |
|    time_elapsed     | 15       |
|    total_timesteps  | 13242    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.03     |
|    n_updates        | 3285     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 256      |
|    fps              | 881      |
|    time_elapsed     | 15       |
|    total_timesteps  | 13327    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 3306     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 260      |
|    fps              | 882      |
|    time_elapsed     | 15       |
|    total_timesteps  | 13406    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0213   |
|    n_updates        | 3326     |
----------------------------------
Eval num_timesteps=13500, episode_reward=4.76 +/- 0.03
Episode length: 24.00 +/- 2.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 24       |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 13500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0688   |
|    n_updates        | 3349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 264      |
|    fps              | 881      |
|    time_elapsed     | 15       |
|    total_timesteps  | 13505    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0868   |
|    n_updates        | 3351     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 268      |
|    fps              | 883      |
|    time_elapsed     | 15       |
|    total_timesteps  | 13592    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0106   |
|    n_updates        | 3372     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 272      |
|    fps              | 884      |
|    time_elapsed     | 15       |
|    total_timesteps  | 13703    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 3400     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 276      |
|    fps              | 886      |
|    time_elapsed     | 15       |
|    total_timesteps  | 13799    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 3424     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 280      |
|    fps              | 887      |
|    time_elapsed     | 15       |
|    total_timesteps  | 13897    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.141    |
|    n_updates        | 3449     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.8     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 284      |
|    fps              | 888      |
|    time_elapsed     | 15       |
|    total_timesteps  | 13989    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.306    |
|    n_updates        | 3472     |
----------------------------------
Eval num_timesteps=14000, episode_reward=4.77 +/- 0.04
Episode length: 23.00 +/- 3.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23       |
|    mean_reward      | 4.77     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 14000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0358   |
|    n_updates        | 3474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 288      |
|    fps              | 887      |
|    time_elapsed     | 15       |
|    total_timesteps  | 14077    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 3494     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 292      |
|    fps              | 889      |
|    time_elapsed     | 15       |
|    total_timesteps  | 14176    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 3518     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 296      |
|    fps              | 890      |
|    time_elapsed     | 16       |
|    total_timesteps  | 14274    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00844  |
|    n_updates        | 3543     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 300      |
|    fps              | 891      |
|    time_elapsed     | 16       |
|    total_timesteps  | 14355    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0599   |
|    n_updates        | 3563     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 304      |
|    fps              | 893      |
|    time_elapsed     | 16       |
|    total_timesteps  | 14444    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0353   |
|    n_updates        | 3585     |
----------------------------------
Eval num_timesteps=14500, episode_reward=4.78 +/- 0.02
Episode length: 22.00 +/- 2.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22       |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 14500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0442   |
|    n_updates        | 3599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.8     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 308      |
|    fps              | 891      |
|    time_elapsed     | 16       |
|    total_timesteps  | 14528    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 3606     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 312      |
|    fps              | 892      |
|    time_elapsed     | 16       |
|    total_timesteps  | 14612    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 3627     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 316      |
|    fps              | 893      |
|    time_elapsed     | 16       |
|    total_timesteps  | 14697    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 3649     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 320      |
|    fps              | 895      |
|    time_elapsed     | 16       |
|    total_timesteps  | 14788    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0279   |
|    n_updates        | 3671     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 324      |
|    fps              | 896      |
|    time_elapsed     | 16       |
|    total_timesteps  | 14882    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 3695     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 328      |
|    fps              | 897      |
|    time_elapsed     | 16       |
|    total_timesteps  | 14981    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 3720     |
----------------------------------
Eval num_timesteps=15000, episode_reward=4.80 +/- 0.02
Episode length: 19.60 +/- 2.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 19.6     |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0462   |
|    n_updates        | 3724     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 332      |
|    fps              | 896      |
|    time_elapsed     | 16       |
|    total_timesteps  | 15073    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0942   |
|    n_updates        | 3743     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 336      |
|    fps              | 897      |
|    time_elapsed     | 16       |
|    total_timesteps  | 15181    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.295    |
|    n_updates        | 3770     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 340      |
|    fps              | 898      |
|    time_elapsed     | 16       |
|    total_timesteps  | 15267    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 3791     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 344      |
|    fps              | 900      |
|    time_elapsed     | 17       |
|    total_timesteps  | 15358    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0561   |
|    n_updates        | 3814     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 348      |
|    fps              | 901      |
|    time_elapsed     | 17       |
|    total_timesteps  | 15453    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.325    |
|    n_updates        | 3838     |
----------------------------------
Eval num_timesteps=15500, episode_reward=4.75 +/- 0.04
Episode length: 25.00 +/- 3.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 25       |
|    mean_reward      | 4.75     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 15500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0526   |
|    n_updates        | 3849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 352      |
|    fps              | 900      |
|    time_elapsed     | 17       |
|    total_timesteps  | 15546    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.36     |
|    n_updates        | 3861     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 356      |
|    fps              | 901      |
|    time_elapsed     | 17       |
|    total_timesteps  | 15632    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0371   |
|    n_updates        | 3882     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 360      |
|    fps              | 902      |
|    time_elapsed     | 17       |
|    total_timesteps  | 15709    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0604   |
|    n_updates        | 3902     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 364      |
|    fps              | 903      |
|    time_elapsed     | 17       |
|    total_timesteps  | 15816    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 3928     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 368      |
|    fps              | 904      |
|    time_elapsed     | 17       |
|    total_timesteps  | 15946    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0516   |
|    n_updates        | 3961     |
----------------------------------
Eval num_timesteps=16000, episode_reward=4.79 +/- 0.02
Episode length: 20.80 +/- 1.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.8     |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 16000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.275    |
|    n_updates        | 3974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 372      |
|    fps              | 904      |
|    time_elapsed     | 17       |
|    total_timesteps  | 16077    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.283    |
|    n_updates        | 3994     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.3     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 376      |
|    fps              | 907      |
|    time_elapsed     | 17       |
|    total_timesteps  | 16333    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0428   |
|    n_updates        | 4058     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26       |
|    ep_rew_mean      | 4.69     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 380      |
|    fps              | 909      |
|    time_elapsed     | 18       |
|    total_timesteps  | 16496    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 4098     |
----------------------------------
Eval num_timesteps=16500, episode_reward=3.44 +/- 2.21
Episode length: 56.40 +/- 23.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.4     |
|    mean_reward      | 3.44     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 16500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0247   |
|    n_updates        | 4099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.8     |
|    ep_rew_mean      | 4.51     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 384      |
|    fps              | 908      |
|    time_elapsed     | 18       |
|    total_timesteps  | 16865    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0193   |
|    n_updates        | 4191     |
----------------------------------
Eval num_timesteps=17000, episode_reward=0.07 +/- 2.08
Episode length: 92.80 +/- 8.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.8     |
|    mean_reward      | 0.072    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 17000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0379   |
|    n_updates        | 4224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.2     |
|    ep_rew_mean      | 4.34     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 388      |
|    fps              | 904      |
|    time_elapsed     | 19       |
|    total_timesteps  | 17197    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.31     |
|    n_updates        | 4274     |
----------------------------------
Eval num_timesteps=17500, episode_reward=2.22 +/- 2.61
Episode length: 77.60 +/- 19.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.6     |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 17500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 4349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.9     |
|    ep_rew_mean      | 4.16     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 392      |
|    fps              | 901      |
|    time_elapsed     | 19       |
|    total_timesteps  | 17569    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 4367     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 396      |
|    fps              | 904      |
|    time_elapsed     | 19       |
|    total_timesteps  | 17879    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 4444     |
----------------------------------
Eval num_timesteps=18000, episode_reward=1.13 +/- 2.57
Episode length: 87.00 +/- 12.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87       |
|    mean_reward      | 1.13     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.289    |
|    n_updates        | 4474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.1     |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 400      |
|    fps              | 901      |
|    time_elapsed     | 20       |
|    total_timesteps  | 18267    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 4541     |
----------------------------------
Eval num_timesteps=18500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00821  |
|    n_updates        | 4599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.1     |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 404      |
|    fps              | 898      |
|    time_elapsed     | 20       |
|    total_timesteps  | 18650    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 4637     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.5     |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 408      |
|    fps              | 901      |
|    time_elapsed     | 21       |
|    total_timesteps  | 18981    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.27     |
|    n_updates        | 4720     |
----------------------------------
Eval num_timesteps=19000, episode_reward=2.28 +/- 2.66
Episode length: 71.80 +/- 22.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.8     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 19000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 4724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.9     |
|    ep_rew_mean      | 3.38     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 412      |
|    fps              | 899      |
|    time_elapsed     | 21       |
|    total_timesteps  | 19298    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00683  |
|    n_updates        | 4799     |
----------------------------------
Eval num_timesteps=19500, episode_reward=0.05 +/- 2.04
Episode length: 94.80 +/- 4.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.8     |
|    mean_reward      | 0.052    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 19500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00368  |
|    n_updates        | 4849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | 3.26     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 416      |
|    fps              | 895      |
|    time_elapsed     | 21       |
|    total_timesteps  | 19614    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 4878     |
----------------------------------
Eval num_timesteps=20000, episode_reward=1.17 +/- 2.62
Episode length: 83.20 +/- 22.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.2     |
|    mean_reward      | 1.17     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00636  |
|    n_updates        | 4974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.1     |
|    ep_rew_mean      | 2.93     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 420      |
|    fps              | 892      |
|    time_elapsed     | 22       |
|    total_timesteps  | 20002    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.184    |
|    n_updates        | 4975     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.7     |
|    ep_rew_mean      | 2.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 424      |
|    fps              | 896      |
|    time_elapsed     | 22       |
|    total_timesteps  | 20349    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.181    |
|    n_updates        | 5062     |
----------------------------------
Eval num_timesteps=20500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 20500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.184    |
|    n_updates        | 5099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.6     |
|    ep_rew_mean      | 2.52     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 428      |
|    fps              | 892      |
|    time_elapsed     | 23       |
|    total_timesteps  | 20737    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0043   |
|    n_updates        | 5159     |
----------------------------------
Eval num_timesteps=21000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 21000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.293    |
|    n_updates        | 5224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.5     |
|    ep_rew_mean      | 2.29     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 432      |
|    fps              | 889      |
|    time_elapsed     | 23       |
|    total_timesteps  | 21125    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.271    |
|    n_updates        | 5256     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.2     |
|    ep_rew_mean      | 2.23     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 436      |
|    fps              | 892      |
|    time_elapsed     | 23       |
|    total_timesteps  | 21401    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.174    |
|    n_updates        | 5325     |
----------------------------------
Eval num_timesteps=21500, episode_reward=0.10 +/- 2.14
Episode length: 89.80 +/- 14.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.8     |
|    mean_reward      | 0.102    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 21500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00328  |
|    n_updates        | 5349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.1     |
|    ep_rew_mean      | 2.05     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 440      |
|    fps              | 890      |
|    time_elapsed     | 24       |
|    total_timesteps  | 21778    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00125  |
|    n_updates        | 5419     |
----------------------------------
Eval num_timesteps=22000, episode_reward=0.09 +/- 2.12
Episode length: 90.80 +/- 12.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.8     |
|    mean_reward      | 0.092    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 22000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.313    |
|    n_updates        | 5474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.1     |
|    ep_rew_mean      | 1.82     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 444      |
|    fps              | 887      |
|    time_elapsed     | 24       |
|    total_timesteps  | 22166    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 5516     |
----------------------------------
Eval num_timesteps=22500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 22500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00145  |
|    n_updates        | 5599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.7     |
|    ep_rew_mean      | 1.69     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 448      |
|    fps              | 884      |
|    time_elapsed     | 25       |
|    total_timesteps  | 22521    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.284    |
|    n_updates        | 5605     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 1.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 452      |
|    fps              | 887      |
|    time_elapsed     | 25       |
|    total_timesteps  | 22844    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.229    |
|    n_updates        | 5685     |
----------------------------------
Eval num_timesteps=23000, episode_reward=0.11 +/- 2.15
Episode length: 89.40 +/- 15.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.4     |
|    mean_reward      | 0.106    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 23000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00149  |
|    n_updates        | 5724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 1.4      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 456      |
|    fps              | 884      |
|    time_elapsed     | 26       |
|    total_timesteps  | 23178    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00251  |
|    n_updates        | 5769     |
----------------------------------
Eval num_timesteps=23500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 23500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00289  |
|    n_updates        | 5849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 1.16     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 460      |
|    fps              | 882      |
|    time_elapsed     | 26       |
|    total_timesteps  | 23566    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00181  |
|    n_updates        | 5866     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 0.992    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 464      |
|    fps              | 885      |
|    time_elapsed     | 26       |
|    total_timesteps  | 23893    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.23     |
|    n_updates        | 5948     |
----------------------------------
Eval num_timesteps=24000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 24000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.238    |
|    n_updates        | 5974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 0.767    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 468      |
|    fps              | 882      |
|    time_elapsed     | 27       |
|    total_timesteps  | 24281    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000967 |
|    n_updates        | 6045     |
----------------------------------
Eval num_timesteps=24500, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 24500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00132  |
|    n_updates        | 6099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.9     |
|    ep_rew_mean      | 0.441    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 472      |
|    fps              | 880      |
|    time_elapsed     | 28       |
|    total_timesteps  | 24669    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.413    |
|    n_updates        | 6142     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00186  |
|    n_updates        | 6224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 0.278    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 476      |
|    fps              | 877      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25057    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00154  |
|    n_updates        | 6239     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.9     |
|    ep_rew_mean      | 0.161    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 480      |
|    fps              | 880      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25389    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.265    |
|    n_updates        | 6322     |
----------------------------------
Eval num_timesteps=25500, episode_reward=0.10 +/- 2.13
Episode length: 90.40 +/- 13.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.4     |
|    mean_reward      | 0.096    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 25500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.271    |
|    n_updates        | 6349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.1     |
|    ep_rew_mean      | 0.0088   |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 484      |
|    fps              | 878      |
|    time_elapsed     | 29       |
|    total_timesteps  | 25777    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00164  |
|    n_updates        | 6419     |
----------------------------------
Eval num_timesteps=26000, episode_reward=-0.97 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | -0.97    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 26000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00212  |
|    n_updates        | 6474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.7     |
|    ep_rew_mean      | -0.147   |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 488      |
|    fps              | 876      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26165    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00158  |
|    n_updates        | 6516     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 492      |
|    fps              | 878      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26489    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00396  |
|    n_updates        | 6597     |
----------------------------------
Eval num_timesteps=26500, episode_reward=-1.92 +/- 4.96
Episode length: 92.00 +/- 10.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92       |
|    mean_reward      | -1.92    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 26500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000723 |
|    n_updates        | 6599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90       |
|    ep_rew_mean      | -0.35    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 496      |
|    fps              | 877      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26877    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.394    |
|    n_updates        | 6694     |
----------------------------------
Eval num_timesteps=27000, episode_reward=2.28 +/- 2.66
Episode length: 71.60 +/- 24.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.6     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 6724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.4     |
|    ep_rew_mean      | -0.344   |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 500      |
|    fps              | 875      |
|    time_elapsed     | 31       |
|    total_timesteps  | 27207    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00251  |
|    n_updates        | 6776     |
----------------------------------
Eval num_timesteps=27500, episode_reward=3.43 +/- 2.20
Episode length: 57.20 +/- 21.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.2     |
|    mean_reward      | 3.43     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 27500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 6849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 504      |
|    fps              | 875      |
|    time_elapsed     | 31       |
|    total_timesteps  | 27520    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00328  |
|    n_updates        | 6854     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.2     |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 508      |
|    fps              | 877      |
|    time_elapsed     | 31       |
|    total_timesteps  | 27803    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00383  |
|    n_updates        | 6925     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | -0.0119  |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 512      |
|    fps              | 878      |
|    time_elapsed     | 31       |
|    total_timesteps  | 27917    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00922  |
|    n_updates        | 6954     |
----------------------------------
Eval num_timesteps=28000, episode_reward=4.61 +/- 0.13
Episode length: 39.20 +/- 12.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.2     |
|    mean_reward      | 4.61     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 28000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00387  |
|    n_updates        | 6974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.6     |
|    ep_rew_mean      | -0.0958  |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 516      |
|    fps              | 877      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28072    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00507  |
|    n_updates        | 6992     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 0.234    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 520      |
|    fps              | 877      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28163    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00202  |
|    n_updates        | 7015     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 0.41     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 524      |
|    fps              | 878      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28249    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00275  |
|    n_updates        | 7037     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 0.64     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 528      |
|    fps              | 879      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28338    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00166  |
|    n_updates        | 7059     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 0.87     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 532      |
|    fps              | 879      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28428    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0016   |
|    n_updates        | 7081     |
----------------------------------
Eval num_timesteps=28500, episode_reward=4.77 +/- 0.03
Episode length: 23.40 +/- 3.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23.4     |
|    mean_reward      | 4.77     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 28500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00498  |
|    n_updates        | 7099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.3     |
|    ep_rew_mean      | 0.937    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 536      |
|    fps              | 879      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28528    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00271  |
|    n_updates        | 7106     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.5     |
|    ep_rew_mean      | 1.12     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 540      |
|    fps              | 879      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28626    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00553  |
|    n_updates        | 7131     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.6     |
|    ep_rew_mean      | 1.34     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 544      |
|    fps              | 880      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28725    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00277  |
|    n_updates        | 7156     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63       |
|    ep_rew_mean      | 1.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 548      |
|    fps              | 881      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28821    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00836  |
|    n_updates        | 7180     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.7     |
|    ep_rew_mean      | 1.59     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 552      |
|    fps              | 881      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28914    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.25     |
|    n_updates        | 7203     |
----------------------------------
Eval num_timesteps=29000, episode_reward=4.77 +/- 0.06
Episode length: 23.40 +/- 6.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23.4     |
|    mean_reward      | 4.77     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 29000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00211  |
|    n_updates        | 7224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.2     |
|    ep_rew_mean      | 1.77     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 556      |
|    fps              | 881      |
|    time_elapsed     | 32       |
|    total_timesteps  | 29001    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00237  |
|    n_updates        | 7225     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.8     |
|    ep_rew_mean      | 1.99     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 560      |
|    fps              | 882      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29145    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00299  |
|    n_updates        | 7261     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54       |
|    ep_rew_mean      | 2.16     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 564      |
|    fps              | 883      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29291    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0188   |
|    n_updates        | 7297     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51       |
|    ep_rew_mean      | 2.39     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 568      |
|    fps              | 884      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29385    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 7321     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48       |
|    ep_rew_mean      | 2.72     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 572      |
|    fps              | 884      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29465    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.274    |
|    n_updates        | 7341     |
----------------------------------
Eval num_timesteps=29500, episode_reward=4.74 +/- 0.05
Episode length: 26.00 +/- 4.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 26       |
|    mean_reward      | 4.74     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 29500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00288  |
|    n_updates        | 7349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45       |
|    ep_rew_mean      | 2.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 576      |
|    fps              | 883      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29562    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.178    |
|    n_updates        | 7365     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.8     |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 580      |
|    fps              | 884      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29671    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.147    |
|    n_updates        | 7392     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.2     |
|    ep_rew_mean      | 3.4      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 584      |
|    fps              | 885      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29793    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00471  |
|    n_updates        | 7423     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.9     |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 588      |
|    fps              | 886      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29955    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 7463     |
----------------------------------
Eval num_timesteps=30000, episode_reward=4.71 +/- 0.05
Episode length: 29.20 +/- 5.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 29.2     |
|    mean_reward      | 4.71     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00459  |
|    n_updates        | 7474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 592      |
|    fps              | 886      |
|    time_elapsed     | 33       |
|    total_timesteps  | 30086    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00262  |
|    n_updates        | 7496     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.6     |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 596      |
|    fps              | 887      |
|    time_elapsed     | 34       |
|    total_timesteps  | 30241    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.242    |
|    n_updates        | 7535     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.6     |
|    ep_rew_mean      | 4.42     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 600      |
|    fps              | 888      |
|    time_elapsed     | 34       |
|    total_timesteps  | 30471    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0016   |
|    n_updates        | 7592     |
----------------------------------
Eval num_timesteps=30500, episode_reward=4.45 +/- 0.17
Episode length: 54.60 +/- 17.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.6     |
|    mean_reward      | 4.45     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 30500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0028   |
|    n_updates        | 7599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.6     |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 604      |
|    fps              | 887      |
|    time_elapsed     | 34       |
|    total_timesteps  | 30776    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00362  |
|    n_updates        | 7668     |
----------------------------------
Eval num_timesteps=31000, episode_reward=0.06 +/- 2.06
Episode length: 94.00 +/- 6.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94       |
|    mean_reward      | 0.06     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 31000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 7724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.3     |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 608      |
|    fps              | 885      |
|    time_elapsed     | 35       |
|    total_timesteps  | 31134    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00321  |
|    n_updates        | 7758     |
----------------------------------
Eval num_timesteps=31500, episode_reward=3.28 +/- 2.13
Episode length: 72.20 +/- 21.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.2     |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 31500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00207  |
|    n_updates        | 7849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 612      |
|    fps              | 884      |
|    time_elapsed     | 35       |
|    total_timesteps  | 31522    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.148    |
|    n_updates        | 7855     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.8     |
|    ep_rew_mean      | 3.82     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 616      |
|    fps              | 886      |
|    time_elapsed     | 35       |
|    total_timesteps  | 31856    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.002    |
|    n_updates        | 7938     |
----------------------------------
Eval num_timesteps=32000, episode_reward=4.32 +/- 0.18
Episode length: 68.40 +/- 18.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.4     |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 32000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00327  |
|    n_updates        | 7974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.4     |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 620      |
|    fps              | 886      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32200    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00146  |
|    n_updates        | 8024     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.8     |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 624      |
|    fps              | 887      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32427    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00095  |
|    n_updates        | 8081     |
----------------------------------
Eval num_timesteps=32500, episode_reward=0.33 +/- 3.77
Episode length: 66.80 +/- 26.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.8     |
|    mean_reward      | 0.332    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 32500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00252  |
|    n_updates        | 8099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.9     |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 628      |
|    fps              | 886      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32727    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.259    |
|    n_updates        | 8156     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.3     |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 632      |
|    fps              | 887      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32959    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0017   |
|    n_updates        | 8214     |
----------------------------------
Eval num_timesteps=33000, episode_reward=2.21 +/- 2.60
Episode length: 78.80 +/- 16.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.8     |
|    mean_reward      | 2.21     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 33000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000777 |
|    n_updates        | 8224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.3     |
|    ep_rew_mean      | 3.29     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 636      |
|    fps              | 885      |
|    time_elapsed     | 37       |
|    total_timesteps  | 33154    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000747 |
|    n_updates        | 8263     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.4     |
|    ep_rew_mean      | 3.28     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 640      |
|    fps              | 886      |
|    time_elapsed     | 37       |
|    total_timesteps  | 33368    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000848 |
|    n_updates        | 8316     |
----------------------------------
Eval num_timesteps=33500, episode_reward=4.32 +/- 0.13
Episode length: 67.60 +/- 13.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.6     |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 33500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000955 |
|    n_updates        | 8349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 644      |
|    fps              | 885      |
|    time_elapsed     | 38       |
|    total_timesteps  | 33693    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00149  |
|    n_updates        | 8398     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.7     |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 648      |
|    fps              | 886      |
|    time_elapsed     | 38       |
|    total_timesteps  | 33889    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000785 |
|    n_updates        | 8447     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.6     |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 652      |
|    fps              | 887      |
|    time_elapsed     | 38       |
|    total_timesteps  | 33978    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00164  |
|    n_updates        | 8469     |
----------------------------------
Eval num_timesteps=34000, episode_reward=4.76 +/- 0.04
Episode length: 24.00 +/- 3.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 24       |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 34000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000755 |
|    n_updates        | 8474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.6     |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 656      |
|    fps              | 886      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34065    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00201  |
|    n_updates        | 8491     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.1     |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 660      |
|    fps              | 887      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34158    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00321  |
|    n_updates        | 8514     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.6     |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 664      |
|    fps              | 887      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34252    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00101  |
|    n_updates        | 8537     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | 3.11     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 668      |
|    fps              | 888      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34331    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00775  |
|    n_updates        | 8557     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 672      |
|    fps              | 888      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34419    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0011   |
|    n_updates        | 8579     |
----------------------------------
Eval num_timesteps=34500, episode_reward=4.77 +/- 0.03
Episode length: 23.00 +/- 3.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23       |
|    mean_reward      | 4.77     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 34500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000625 |
|    n_updates        | 8599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 676      |
|    fps              | 888      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34512    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.247    |
|    n_updates        | 8602     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | 3.11     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 680      |
|    fps              | 888      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34604    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00157  |
|    n_updates        | 8625     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | 3.11     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 684      |
|    fps              | 889      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34700    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 8649     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.6     |
|    ep_rew_mean      | 3.11     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 688      |
|    fps              | 890      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34816    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.28     |
|    n_updates        | 8678     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.2     |
|    ep_rew_mean      | 3.12     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 692      |
|    fps              | 890      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34903    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00182  |
|    n_updates        | 8700     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.5     |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 696      |
|    fps              | 891      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34990    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00138  |
|    n_updates        | 8722     |
----------------------------------
Eval num_timesteps=35000, episode_reward=4.76 +/- 0.03
Episode length: 23.60 +/- 3.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23.6     |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.249    |
|    n_updates        | 8724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.1     |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 700      |
|    fps              | 890      |
|    time_elapsed     | 39       |
|    total_timesteps  | 35077    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000748 |
|    n_updates        | 8744     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.9     |
|    ep_rew_mean      | 3.26     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 704      |
|    fps              | 891      |
|    time_elapsed     | 39       |
|    total_timesteps  | 35169    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00125  |
|    n_updates        | 8767     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.4     |
|    ep_rew_mean      | 3.64     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 708      |
|    fps              | 891      |
|    time_elapsed     | 39       |
|    total_timesteps  | 35272    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00204  |
|    n_updates        | 8792     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.4     |
|    ep_rew_mean      | 3.87     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 712      |
|    fps              | 892      |
|    time_elapsed     | 39       |
|    total_timesteps  | 35360    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00496  |
|    n_updates        | 8814     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.1     |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 716      |
|    fps              | 892      |
|    time_elapsed     | 39       |
|    total_timesteps  | 35464    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00105  |
|    n_updates        | 8840     |
----------------------------------
Eval num_timesteps=35500, episode_reward=1.31 +/- 2.79
Episode length: 69.00 +/- 34.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69       |
|    mean_reward      | 1.31     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 35500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00149  |
|    n_updates        | 8849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.8     |
|    ep_rew_mean      | 4.16     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 720      |
|    fps              | 890      |
|    time_elapsed     | 39       |
|    total_timesteps  | 35584    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00187  |
|    n_updates        | 8870     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.5     |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 724      |
|    fps              | 891      |
|    time_elapsed     | 40       |
|    total_timesteps  | 35672    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00116  |
|    n_updates        | 8892     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.9     |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 728      |
|    fps              | 892      |
|    time_elapsed     | 40       |
|    total_timesteps  | 35818    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00234  |
|    n_updates        | 8929     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.5     |
|    ep_rew_mean      | 4.56     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 732      |
|    fps              | 892      |
|    time_elapsed     | 40       |
|    total_timesteps  | 35908    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00244  |
|    n_updates        | 8951     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.4     |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 736      |
|    fps              | 893      |
|    time_elapsed     | 40       |
|    total_timesteps  | 35996    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000617 |
|    n_updates        | 8973     |
----------------------------------
Eval num_timesteps=36000, episode_reward=4.77 +/- 0.06
Episode length: 23.20 +/- 6.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23.2     |
|    mean_reward      | 4.77     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00155  |
|    n_updates        | 8974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.2     |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 740      |
|    fps              | 892      |
|    time_elapsed     | 40       |
|    total_timesteps  | 36087    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00165  |
|    n_updates        | 8996     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 744      |
|    fps              | 892      |
|    time_elapsed     | 40       |
|    total_timesteps  | 36180    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0015   |
|    n_updates        | 9019     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.8     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 748      |
|    fps              | 893      |
|    time_elapsed     | 40       |
|    total_timesteps  | 36264    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 9040     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 752      |
|    fps              | 893      |
|    time_elapsed     | 40       |
|    total_timesteps  | 36349    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00114  |
|    n_updates        | 9062     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.8     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 756      |
|    fps              | 894      |
|    time_elapsed     | 40       |
|    total_timesteps  | 36440    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00223  |
|    n_updates        | 9084     |
----------------------------------
Eval num_timesteps=36500, episode_reward=4.73 +/- 0.03
Episode length: 27.00 +/- 3.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 27       |
|    mean_reward      | 4.73     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 36500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 9099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 760      |
|    fps              | 893      |
|    time_elapsed     | 40       |
|    total_timesteps  | 36530    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.002    |
|    n_updates        | 9107     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 764      |
|    fps              | 894      |
|    time_elapsed     | 41       |
|    total_timesteps  | 36698    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00333  |
|    n_updates        | 9149     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 768      |
|    fps              | 895      |
|    time_elapsed     | 41       |
|    total_timesteps  | 36843    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00225  |
|    n_updates        | 9185     |
----------------------------------
Eval num_timesteps=37000, episode_reward=4.55 +/- 0.11
Episode length: 45.40 +/- 11.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 4.55     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 37000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.254    |
|    n_updates        | 9224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26       |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 772      |
|    fps              | 894      |
|    time_elapsed     | 41       |
|    total_timesteps  | 37019    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00263  |
|    n_updates        | 9229     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.2     |
|    ep_rew_mean      | 4.63     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 776      |
|    fps              | 895      |
|    time_elapsed     | 41       |
|    total_timesteps  | 37228    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000772 |
|    n_updates        | 9281     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.4     |
|    ep_rew_mean      | 4.63     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 780      |
|    fps              | 896      |
|    time_elapsed     | 41       |
|    total_timesteps  | 37340    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00142  |
|    n_updates        | 9309     |
----------------------------------
Eval num_timesteps=37500, episode_reward=0.11 +/- 2.16
Episode length: 89.00 +/- 16.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89       |
|    mean_reward      | 0.11     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 37500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00196  |
|    n_updates        | 9349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.6     |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 784      |
|    fps              | 894      |
|    time_elapsed     | 42       |
|    total_timesteps  | 37656    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.002    |
|    n_updates        | 9388     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.1     |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 788      |
|    fps              | 895      |
|    time_elapsed     | 42       |
|    total_timesteps  | 37826    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000906 |
|    n_updates        | 9431     |
----------------------------------
Eval num_timesteps=38000, episode_reward=4.35 +/- 0.17
Episode length: 64.60 +/- 17.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.6     |
|    mean_reward      | 4.35     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 38000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00137  |
|    n_updates        | 9474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.8     |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 792      |
|    fps              | 894      |
|    time_elapsed     | 42       |
|    total_timesteps  | 38085    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000917 |
|    n_updates        | 9496     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 796      |
|    fps              | 894      |
|    time_elapsed     | 42       |
|    total_timesteps  | 38182    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000576 |
|    n_updates        | 9520     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.3     |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 800      |
|    fps              | 895      |
|    time_elapsed     | 42       |
|    total_timesteps  | 38310    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00131  |
|    n_updates        | 9552     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.5     |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 804      |
|    fps              | 895      |
|    time_elapsed     | 42       |
|    total_timesteps  | 38416    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00112  |
|    n_updates        | 9578     |
----------------------------------
Eval num_timesteps=38500, episode_reward=4.55 +/- 0.21
Episode length: 45.00 +/- 20.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 4.55     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 38500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00132  |
|    n_updates        | 9599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.1     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 808      |
|    fps              | 895      |
|    time_elapsed     | 43       |
|    total_timesteps  | 38583    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 9620     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.2     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 812      |
|    fps              | 895      |
|    time_elapsed     | 43       |
|    total_timesteps  | 38678    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000721 |
|    n_updates        | 9644     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.1     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 816      |
|    fps              | 895      |
|    time_elapsed     | 43       |
|    total_timesteps  | 38774    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000871 |
|    n_updates        | 9668     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.9     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 820      |
|    fps              | 896      |
|    time_elapsed     | 43       |
|    total_timesteps  | 38870    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00112  |
|    n_updates        | 9692     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.8     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 824      |
|    fps              | 896      |
|    time_elapsed     | 43       |
|    total_timesteps  | 38956    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000265 |
|    n_updates        | 9713     |
----------------------------------
Eval num_timesteps=39000, episode_reward=4.79 +/- 0.02
Episode length: 20.60 +/- 1.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.6     |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 39000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000372 |
|    n_updates        | 9724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.4     |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 828      |
|    fps              | 896      |
|    time_elapsed     | 43       |
|    total_timesteps  | 39060    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000383 |
|    n_updates        | 9739     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.8     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 832      |
|    fps              | 897      |
|    time_elapsed     | 43       |
|    total_timesteps  | 39192    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000837 |
|    n_updates        | 9772     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.8     |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 836      |
|    fps              | 897      |
|    time_elapsed     | 43       |
|    total_timesteps  | 39279    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00493  |
|    n_updates        | 9794     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.7     |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 840      |
|    fps              | 897      |
|    time_elapsed     | 43       |
|    total_timesteps  | 39356    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000674 |
|    n_updates        | 9813     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33       |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 844      |
|    fps              | 898      |
|    time_elapsed     | 43       |
|    total_timesteps  | 39475    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.243    |
|    n_updates        | 9843     |
----------------------------------
Eval num_timesteps=39500, episode_reward=2.71 +/- 4.01
Episode length: 28.60 +/- 3.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 28.6     |
|    mean_reward      | 2.71     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 39500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000245 |
|    n_updates        | 9849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.1     |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 848      |
|    fps              | 898      |
|    time_elapsed     | 44       |
|    total_timesteps  | 39573    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000493 |
|    n_updates        | 9868     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33       |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 852      |
|    fps              | 898      |
|    time_elapsed     | 44       |
|    total_timesteps  | 39654    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000323 |
|    n_updates        | 9888     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33       |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 856      |
|    fps              | 898      |
|    time_elapsed     | 44       |
|    total_timesteps  | 39743    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000538 |
|    n_updates        | 9910     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.1     |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 860      |
|    fps              | 899      |
|    time_elapsed     | 44       |
|    total_timesteps  | 39845    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00034  |
|    n_updates        | 9936     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.4     |
|    ep_rew_mean      | 4.38     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 864      |
|    fps              | 899      |
|    time_elapsed     | 44       |
|    total_timesteps  | 39940    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.241    |
|    n_updates        | 9959     |
----------------------------------
Eval num_timesteps=40000, episode_reward=4.79 +/- 0.01
Episode length: 20.80 +/- 0.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.8     |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000535 |
|    n_updates        | 9974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32       |
|    ep_rew_mean      | 4.38     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 868      |
|    fps              | 899      |
|    time_elapsed     | 44       |
|    total_timesteps  | 40047    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.211    |
|    n_updates        | 9986     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.3     |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 872      |
|    fps              | 900      |
|    time_elapsed     | 44       |
|    total_timesteps  | 40150    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.001    |
|    n_updates        | 10012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.7     |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 876      |
|    fps              | 900      |
|    time_elapsed     | 44       |
|    total_timesteps  | 40296    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.238    |
|    n_updates        | 10048    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.4     |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 880      |
|    fps              | 901      |
|    time_elapsed     | 44       |
|    total_timesteps  | 40385    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 10071    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.2     |
|    ep_rew_mean      | 4.62     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 884      |
|    fps              | 901      |
|    time_elapsed     | 44       |
|    total_timesteps  | 40477    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00084  |
|    n_updates        | 10094    |
----------------------------------
Eval num_timesteps=40500, episode_reward=4.79 +/- 0.01
Episode length: 21.00 +/- 1.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21       |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 10099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.5     |
|    ep_rew_mean      | 4.63     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 888      |
|    fps              | 901      |
|    time_elapsed     | 45       |
|    total_timesteps  | 40572    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000491 |
|    n_updates        | 10117    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.8     |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 892      |
|    fps              | 901      |
|    time_elapsed     | 45       |
|    total_timesteps  | 40663    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 10140    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.8     |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 896      |
|    fps              | 902      |
|    time_elapsed     | 45       |
|    total_timesteps  | 40759    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.226    |
|    n_updates        | 10164    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.7     |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 900      |
|    fps              | 902      |
|    time_elapsed     | 45       |
|    total_timesteps  | 40884    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 10195    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.5     |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 904      |
|    fps              | 903      |
|    time_elapsed     | 45       |
|    total_timesteps  | 40968    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000403 |
|    n_updates        | 10216    |
----------------------------------
Eval num_timesteps=41000, episode_reward=4.77 +/- 0.03
Episode length: 23.20 +/- 3.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23.2     |
|    mean_reward      | 4.77     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00374  |
|    n_updates        | 10224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 4.55     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 908      |
|    fps              | 902      |
|    time_elapsed     | 45       |
|    total_timesteps  | 41072    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000747 |
|    n_updates        | 10242    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 912      |
|    fps              | 903      |
|    time_elapsed     | 45       |
|    total_timesteps  | 41158    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 10264    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25       |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 916      |
|    fps              | 903      |
|    time_elapsed     | 45       |
|    total_timesteps  | 41278    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000312 |
|    n_updates        | 10294    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25       |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 920      |
|    fps              | 904      |
|    time_elapsed     | 45       |
|    total_timesteps  | 41366    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00168  |
|    n_updates        | 10316    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 924      |
|    fps              | 904      |
|    time_elapsed     | 45       |
|    total_timesteps  | 41471    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00039  |
|    n_updates        | 10342    |
----------------------------------
Eval num_timesteps=41500, episode_reward=0.70 +/- 5.01
Episode length: 30.20 +/- 14.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 30.2     |
|    mean_reward      | 0.698    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000281 |
|    n_updates        | 10349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25       |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 928      |
|    fps              | 903      |
|    time_elapsed     | 45       |
|    total_timesteps  | 41557    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000443 |
|    n_updates        | 10364    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 932      |
|    fps              | 904      |
|    time_elapsed     | 46       |
|    total_timesteps  | 41645    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00215  |
|    n_updates        | 10386    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 936      |
|    fps              | 904      |
|    time_elapsed     | 46       |
|    total_timesteps  | 41733    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000454 |
|    n_updates        | 10408    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25       |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 940      |
|    fps              | 905      |
|    time_elapsed     | 46       |
|    total_timesteps  | 41857    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000381 |
|    n_updates        | 10439    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 944      |
|    fps              | 905      |
|    time_elapsed     | 46       |
|    total_timesteps  | 41949    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00103  |
|    n_updates        | 10462    |
----------------------------------
Eval num_timesteps=42000, episode_reward=4.77 +/- 0.03
Episode length: 22.80 +/- 2.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22.8     |
|    mean_reward      | 4.77     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000243 |
|    n_updates        | 10474    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 948      |
|    fps              | 905      |
|    time_elapsed     | 46       |
|    total_timesteps  | 42039    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00068  |
|    n_updates        | 10484    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 952      |
|    fps              | 905      |
|    time_elapsed     | 46       |
|    total_timesteps  | 42132    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 10507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 956      |
|    fps              | 906      |
|    time_elapsed     | 46       |
|    total_timesteps  | 42212    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000458 |
|    n_updates        | 10527    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.6     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 960      |
|    fps              | 906      |
|    time_elapsed     | 46       |
|    total_timesteps  | 42309    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.273    |
|    n_updates        | 10552    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 964      |
|    fps              | 906      |
|    time_elapsed     | 46       |
|    total_timesteps  | 42409    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000279 |
|    n_updates        | 10577    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 968      |
|    fps              | 907      |
|    time_elapsed     | 46       |
|    total_timesteps  | 42495    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000185 |
|    n_updates        | 10598    |
----------------------------------
Eval num_timesteps=42500, episode_reward=4.77 +/- 0.04
Episode length: 22.80 +/- 4.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22.8     |
|    mean_reward      | 4.77     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00015  |
|    n_updates        | 10599    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.4     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 972      |
|    fps              | 906      |
|    time_elapsed     | 46       |
|    total_timesteps  | 42589    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000217 |
|    n_updates        | 10622    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 976      |
|    fps              | 907      |
|    time_elapsed     | 47       |
|    total_timesteps  | 42704    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 10650    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 980      |
|    fps              | 907      |
|    time_elapsed     | 47       |
|    total_timesteps  | 42797    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000242 |
|    n_updates        | 10674    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 984      |
|    fps              | 908      |
|    time_elapsed     | 47       |
|    total_timesteps  | 42895    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00024  |
|    n_updates        | 10698    |
----------------------------------
Eval num_timesteps=43000, episode_reward=4.78 +/- 0.01
Episode length: 21.60 +/- 1.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.6     |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000174 |
|    n_updates        | 10724    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 988      |
|    fps              | 908      |
|    time_elapsed     | 47       |
|    total_timesteps  | 43001    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00022  |
|    n_updates        | 10725    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 992      |
|    fps              | 908      |
|    time_elapsed     | 47       |
|    total_timesteps  | 43082    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000965 |
|    n_updates        | 10745    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 996      |
|    fps              | 908      |
|    time_elapsed     | 47       |
|    total_timesteps  | 43174    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 10768    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 909      |
|    time_elapsed     | 47       |
|    total_timesteps  | 43272    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000791 |
|    n_updates        | 10792    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 909      |
|    time_elapsed     | 47       |
|    total_timesteps  | 43374    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.342    |
|    n_updates        | 10818    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 910      |
|    time_elapsed     | 47       |
|    total_timesteps  | 43471    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000142 |
|    n_updates        | 10842    |
----------------------------------
Eval num_timesteps=43500, episode_reward=4.79 +/- 0.02
Episode length: 20.80 +/- 1.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.8     |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000122 |
|    n_updates        | 10849    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | 4.66     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 909      |
|    time_elapsed     | 47       |
|    total_timesteps  | 43561    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000184 |
|    n_updates        | 10865    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.66     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 909      |
|    time_elapsed     | 47       |
|    total_timesteps  | 43645    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000952 |
|    n_updates        | 10886    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.66     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 910      |
|    time_elapsed     | 48       |
|    total_timesteps  | 43735    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000153 |
|    n_updates        | 10908    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | 4.67     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 910      |
|    time_elapsed     | 48       |
|    total_timesteps  | 43815    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000175 |
|    n_updates        | 10928    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | 4.67     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 910      |
|    time_elapsed     | 48       |
|    total_timesteps  | 43892    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000733 |
|    n_updates        | 10947    |
----------------------------------
Eval num_timesteps=44000, episode_reward=4.80 +/- 0.02
Episode length: 19.60 +/- 1.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 19.6     |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000943 |
|    n_updates        | 10974    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.66     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 910      |
|    time_elapsed     | 48       |
|    total_timesteps  | 44000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 4.66     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 910      |
|    time_elapsed     | 48       |
|    total_timesteps  | 44104    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000665 |
|    n_updates        | 11000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.3     |
|    ep_rew_mean      | 4.67     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 911      |
|    time_elapsed     | 48       |
|    total_timesteps  | 44185    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000136 |
|    n_updates        | 11021    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 911      |
|    time_elapsed     | 48       |
|    total_timesteps  | 44289    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000126 |
|    n_updates        | 11047    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.56     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 912      |
|    time_elapsed     | 48       |
|    total_timesteps  | 44404    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000145 |
|    n_updates        | 11075    |
----------------------------------
Eval num_timesteps=44500, episode_reward=4.78 +/- 0.02
Episode length: 22.40 +/- 2.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22.4     |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 11099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.8     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 911      |
|    time_elapsed     | 48       |
|    total_timesteps  | 44515    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 11103    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 912      |
|    time_elapsed     | 48       |
|    total_timesteps  | 44640    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.64e-05 |
|    n_updates        | 11134    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 912      |
|    time_elapsed     | 49       |
|    total_timesteps  | 44735    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00035  |
|    n_updates        | 11158    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 913      |
|    time_elapsed     | 49       |
|    total_timesteps  | 44827    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.202    |
|    n_updates        | 11181    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 913      |
|    time_elapsed     | 49       |
|    total_timesteps  | 44916    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000114 |
|    n_updates        | 11203    |
----------------------------------
Eval num_timesteps=45000, episode_reward=4.75 +/- 0.05
Episode length: 25.20 +/- 5.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 25.2     |
|    mean_reward      | 4.75     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000106 |
|    n_updates        | 11224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.4     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 913      |
|    time_elapsed     | 49       |
|    total_timesteps  | 45026    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 11231    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 913      |
|    time_elapsed     | 49       |
|    total_timesteps  | 45113    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000112 |
|    n_updates        | 11253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 913      |
|    time_elapsed     | 49       |
|    total_timesteps  | 45216    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00103  |
|    n_updates        | 11278    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 914      |
|    time_elapsed     | 49       |
|    total_timesteps  | 45347    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00145  |
|    n_updates        | 11311    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.4     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 914      |
|    time_elapsed     | 49       |
|    total_timesteps  | 45441    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0002   |
|    n_updates        | 11335    |
----------------------------------
Eval num_timesteps=45500, episode_reward=2.67 +/- 3.98
Episode length: 33.20 +/- 6.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.2     |
|    mean_reward      | 2.67     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000121 |
|    n_updates        | 11349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.6     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 914      |
|    time_elapsed     | 49       |
|    total_timesteps  | 45539    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000176 |
|    n_updates        | 11359    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25       |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 914      |
|    time_elapsed     | 49       |
|    total_timesteps  | 45673    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00032  |
|    n_updates        | 11393    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 914      |
|    time_elapsed     | 50       |
|    total_timesteps  | 45754    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000667 |
|    n_updates        | 11413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 915      |
|    time_elapsed     | 50       |
|    total_timesteps  | 45856    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00025  |
|    n_updates        | 11438    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 915      |
|    time_elapsed     | 50       |
|    total_timesteps  | 45949    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000596 |
|    n_updates        | 11462    |
----------------------------------
Eval num_timesteps=46000, episode_reward=4.80 +/- 0.02
Episode length: 20.00 +/- 2.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20       |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 11474    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | 4.15     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 915      |
|    time_elapsed     | 50       |
|    total_timesteps  | 46033    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 11483    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 915      |
|    time_elapsed     | 50       |
|    total_timesteps  | 46122    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000928 |
|    n_updates        | 11505    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 915      |
|    time_elapsed     | 50       |
|    total_timesteps  | 46221    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000172 |
|    n_updates        | 11530    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 916      |
|    time_elapsed     | 50       |
|    total_timesteps  | 46305    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00018  |
|    n_updates        | 11551    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25       |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 916      |
|    time_elapsed     | 50       |
|    total_timesteps  | 46391    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 11572    |
----------------------------------
Eval num_timesteps=46500, episode_reward=2.77 +/- 3.99
Episode length: 23.00 +/- 3.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23       |
|    mean_reward      | 2.77     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000119 |
|    n_updates        | 11599    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25       |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 916      |
|    time_elapsed     | 50       |
|    total_timesteps  | 46500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 916      |
|    time_elapsed     | 50       |
|    total_timesteps  | 46593    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000264 |
|    n_updates        | 11623    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 916      |
|    time_elapsed     | 50       |
|    total_timesteps  | 46678    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.2      |
|    n_updates        | 11644    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 4.15     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 917      |
|    time_elapsed     | 50       |
|    total_timesteps  | 46774    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000153 |
|    n_updates        | 11668    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 917      |
|    time_elapsed     | 51       |
|    total_timesteps  | 46883    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000235 |
|    n_updates        | 11695    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.6     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 918      |
|    time_elapsed     | 51       |
|    total_timesteps  | 46976    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 7.67e-05 |
|    n_updates        | 11718    |
----------------------------------
Eval num_timesteps=47000, episode_reward=4.79 +/- 0.03
Episode length: 21.20 +/- 2.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.2     |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.48e-05 |
|    n_updates        | 11724    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 917      |
|    time_elapsed     | 51       |
|    total_timesteps  | 47055    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0473   |
|    n_updates        | 11738    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 918      |
|    time_elapsed     | 51       |
|    total_timesteps  | 47152    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000201 |
|    n_updates        | 11762    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 918      |
|    time_elapsed     | 51       |
|    total_timesteps  | 47245    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 11786    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 918      |
|    time_elapsed     | 51       |
|    total_timesteps  | 47338    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00027  |
|    n_updates        | 11809    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 919      |
|    time_elapsed     | 51       |
|    total_timesteps  | 47429    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.259    |
|    n_updates        | 11832    |
----------------------------------
Eval num_timesteps=47500, episode_reward=4.78 +/- 0.03
Episode length: 21.60 +/- 2.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.6     |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00974  |
|    n_updates        | 11849    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 918      |
|    time_elapsed     | 51       |
|    total_timesteps  | 47519    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000773 |
|    n_updates        | 11854    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 919      |
|    time_elapsed     | 51       |
|    total_timesteps  | 47608    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 11876    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 919      |
|    time_elapsed     | 51       |
|    total_timesteps  | 47697    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.256    |
|    n_updates        | 11899    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 919      |
|    time_elapsed     | 51       |
|    total_timesteps  | 47783    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000149 |
|    n_updates        | 11920    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.3     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 919      |
|    time_elapsed     | 52       |
|    total_timesteps  | 47866    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00014  |
|    n_updates        | 11941    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 920      |
|    time_elapsed     | 52       |
|    total_timesteps  | 47958    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.82e-05 |
|    n_updates        | 11964    |
----------------------------------
Eval num_timesteps=48000, episode_reward=4.79 +/- 0.03
Episode length: 21.40 +/- 2.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.4     |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000134 |
|    n_updates        | 11974    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 919      |
|    time_elapsed     | 52       |
|    total_timesteps  | 48054    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000326 |
|    n_updates        | 11988    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 920      |
|    time_elapsed     | 52       |
|    total_timesteps  | 48152    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 12012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 920      |
|    time_elapsed     | 52       |
|    total_timesteps  | 48244    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000401 |
|    n_updates        | 12035    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.67     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 920      |
|    time_elapsed     | 52       |
|    total_timesteps  | 48345    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000168 |
|    n_updates        | 12061    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 921      |
|    time_elapsed     | 52       |
|    total_timesteps  | 48439    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00058  |
|    n_updates        | 12084    |
----------------------------------
Eval num_timesteps=48500, episode_reward=4.80 +/- 0.02
Episode length: 20.20 +/- 2.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.2     |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0369   |
|    n_updates        | 12099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 921      |
|    time_elapsed     | 52       |
|    total_timesteps  | 48531    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000457 |
|    n_updates        | 12107    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 921      |
|    time_elapsed     | 52       |
|    total_timesteps  | 48622    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000168 |
|    n_updates        | 12130    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.3     |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 921      |
|    time_elapsed     | 52       |
|    total_timesteps  | 48717    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000209 |
|    n_updates        | 12154    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 922      |
|    time_elapsed     | 52       |
|    total_timesteps  | 48815    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000219 |
|    n_updates        | 12178    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 922      |
|    time_elapsed     | 53       |
|    total_timesteps  | 48910    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000245 |
|    n_updates        | 12202    |
----------------------------------
Eval num_timesteps=49000, episode_reward=4.78 +/- 0.01
Episode length: 22.00 +/- 1.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22       |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000156 |
|    n_updates        | 12224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 922      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49002    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 12225    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 922      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49092    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000246 |
|    n_updates        | 12247    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 922      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49193    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000172 |
|    n_updates        | 12273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 923      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49281    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000347 |
|    n_updates        | 12295    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 923      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49369    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00345  |
|    n_updates        | 12317    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 923      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49461    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000184 |
|    n_updates        | 12340    |
----------------------------------
Eval num_timesteps=49500, episode_reward=4.78 +/- 0.03
Episode length: 21.60 +/- 3.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.6     |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000223 |
|    n_updates        | 12349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 923      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49557    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000858 |
|    n_updates        | 12364    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 923      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49660    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000642 |
|    n_updates        | 12389    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 923      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49736    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.168    |
|    n_updates        | 12408    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.3     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 924      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49845    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000201 |
|    n_updates        | 12436    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 924      |
|    time_elapsed     | 53       |
|    total_timesteps  | 49923    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000246 |
|    n_updates        | 12455    |
----------------------------------
Eval num_timesteps=50000, episode_reward=4.79 +/- 0.01
Episode length: 20.60 +/- 0.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.6     |
|    mean_reward      | 4.79     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000154 |
|    n_updates        | 12474    |
----------------------------------
 100%  50,000/50,000  [ 0:00:54 < 0:00:00 , 954 it/s ]
1283
1283
[[[1. 0. 0. ... 0. 0. 0.]]

 [[1. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 ...

 [[0. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]

 [[0. 0. 0. ... 0. 0. 0.]]]
Agent: [0 0]
Target: [9 9]
Action: 1
location: [0 0]
Action: 0
location: [0 1]
Action: 3
location: [1 1]
Action: 3
location: [2 1]
Action: 3
location: [3 1]
Action: 0
location: [3 2]
Action: 0
location: [3 3]
Action: 0
location: [3 4]
Action: 1
location: [2 4]
Action: 0
location: [2 5]
Action: 0
location: [2 6]
Action: 0
location: [2 7]
Action: 3
location: [3 7]
Action: 2
location: [3 6]
Action: 0
location: [3 7]
Action: 3
location: [4 7]
Action: 3
location: [5 7]
Action: 0
location: [5 8]
Action: 3
location: [6 8]
Action: 3
location: [7 8]
Action: 3
location: [8 8]
Action: 0
location: [8 9]
Action: 3
location: [9 9]
Completed in 22 steps with score of 4.99
(9, 9)
(9, 9)
['explored  ', 'explored  ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['          ', 'explored  ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['          ', 'explored  ', '          ', '          ', 'explored  ', 'explored  ', 'explored  ', 'explored  ', '          ', '          ']
['          ', 'explored  ', 'explored  ', 'explored  ', 'explored  ', '          ', 'explored  ', 'explored  ', '          ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', '          ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', 'explored  ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'agent     ']
Title: dqn path
Save path: ./graphs/dqnpath.pngdqn path
Execution time: 55.79869365692139 seconds
