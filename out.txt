pygame 2.1.3 (SDL 2.0.22, Python 3.11.1)
Hello from the pygame community. https://www.pygame.org/contribute.html
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Testing algo: dqn
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -33.8    |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 802      |
|    time_elapsed     | 0        |
|    total_timesteps  | 388      |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.417    |
|    n_updates        | 71       |
----------------------------------
Eval num_timesteps=500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.92     |
| time/               |          |
|    total_timesteps  | 500      |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.279    |
|    n_updates        | 99       |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -22.5    |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 787      |
|    time_elapsed     | 0        |
|    total_timesteps  | 776      |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.419    |
|    n_updates        | 168      |
----------------------------------
Eval num_timesteps=1000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.84     |
| time/               |          |
|    total_timesteps  | 1000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000664 |
|    n_updates        | 224      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -15      |
|    exploration_rate | 0.814    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 785      |
|    time_elapsed     | 1        |
|    total_timesteps  | 1164     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 265      |
----------------------------------
Eval num_timesteps=1500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.76     |
| time/               |          |
|    total_timesteps  | 1500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000696 |
|    n_updates        | 349      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | -10.6    |
|    exploration_rate | 0.753    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 778      |
|    time_elapsed     | 1        |
|    total_timesteps  | 1541     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 360      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | -10.5    |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 844      |
|    time_elapsed     | 2        |
|    total_timesteps  | 1929     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.276    |
|    n_updates        | 457      |
----------------------------------
Eval num_timesteps=2000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.68     |
| time/               |          |
|    total_timesteps  | 2000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000221 |
|    n_updates        | 474      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | -8.96    |
|    exploration_rate | 0.629    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 826      |
|    time_elapsed     | 2        |
|    total_timesteps  | 2317     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 554      |
----------------------------------
Eval num_timesteps=2500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.6      |
| time/               |          |
|    total_timesteps  | 2500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000198 |
|    n_updates        | 599      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | -8.21    |
|    exploration_rate | 0.569    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 809      |
|    time_elapsed     | 3        |
|    total_timesteps  | 2691     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 647      |
----------------------------------
Eval num_timesteps=3000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.52     |
| time/               |          |
|    total_timesteps  | 3000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000113 |
|    n_updates        | 724      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | -7.5     |
|    exploration_rate | 0.507    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 797      |
|    time_elapsed     | 3        |
|    total_timesteps  | 3079     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 744      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | -6.53    |
|    exploration_rate | 0.451    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 822      |
|    time_elapsed     | 4        |
|    total_timesteps  | 3432     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000162 |
|    n_updates        | 832      |
----------------------------------
Eval num_timesteps=3500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.44     |
| time/               |          |
|    total_timesteps  | 3500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000229 |
|    n_updates        | 849      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | -5.88    |
|    exploration_rate | 0.389    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 806      |
|    time_elapsed     | 4        |
|    total_timesteps  | 3820     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000268 |
|    n_updates        | 929      |
----------------------------------
Eval num_timesteps=4000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.36     |
| time/               |          |
|    total_timesteps  | 4000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000131 |
|    n_updates        | 974      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | -5.34    |
|    exploration_rate | 0.327    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 791      |
|    time_elapsed     | 5        |
|    total_timesteps  | 4208     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 1026     |
----------------------------------
Eval num_timesteps=4500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.28     |
| time/               |          |
|    total_timesteps  | 4500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000212 |
|    n_updates        | 1099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | -5.21    |
|    exploration_rate | 0.265    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 779      |
|    time_elapsed     | 5        |
|    total_timesteps  | 4596     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.03e-05 |
|    n_updates        | 1123     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | -4.81    |
|    exploration_rate | 0.203    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 795      |
|    time_elapsed     | 6        |
|    total_timesteps  | 4984     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 1220     |
----------------------------------
Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 5000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.75e-05 |
|    n_updates        | 1224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | -4.38    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 56       |
|    fps              | 781      |
|    time_elapsed     | 6        |
|    total_timesteps  | 5335     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000109 |
|    n_updates        | 1308     |
----------------------------------
Eval num_timesteps=5500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 5500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 1349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | -4.08    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 60       |
|    fps              | 772      |
|    time_elapsed     | 7        |
|    total_timesteps  | 5723     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000121 |
|    n_updates        | 1405     |
----------------------------------
Eval num_timesteps=6000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 6000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 7.83e-05 |
|    n_updates        | 1474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | -3.83    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 64       |
|    fps              | 763      |
|    time_elapsed     | 8        |
|    total_timesteps  | 6111     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.15e-05 |
|    n_updates        | 1502     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | -3.6     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 68       |
|    fps              | 775      |
|    time_elapsed     | 8        |
|    total_timesteps  | 6499     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4e-05    |
|    n_updates        | 1599     |
----------------------------------
Eval num_timesteps=6500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6500     |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | -3.4     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 72       |
|    fps              | 766      |
|    time_elapsed     | 8        |
|    total_timesteps  | 6887     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00018  |
|    n_updates        | 1696     |
----------------------------------
Eval num_timesteps=7000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 7000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 1724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | -3.22    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 76       |
|    fps              | 759      |
|    time_elapsed     | 9        |
|    total_timesteps  | 7275     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 9.38e-05 |
|    n_updates        | 1793     |
----------------------------------
Eval num_timesteps=7500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 7500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.98e-05 |
|    n_updates        | 1849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | -3.06    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 80       |
|    fps              | 752      |
|    time_elapsed     | 10       |
|    total_timesteps  | 7663     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.87e-05 |
|    n_updates        | 1890     |
----------------------------------
Eval num_timesteps=8000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 8000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 7.64e-05 |
|    n_updates        | 1974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | -2.92    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 84       |
|    fps              | 746      |
|    time_elapsed     | 10       |
|    total_timesteps  | 8051     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 7.31e-05 |
|    n_updates        | 1987     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | -2.78    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 88       |
|    fps              | 756      |
|    time_elapsed     | 11       |
|    total_timesteps  | 8439     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000135 |
|    n_updates        | 2084     |
----------------------------------
Eval num_timesteps=8500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 8500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.57e-05 |
|    n_updates        | 2099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | -2.66    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 92       |
|    fps              | 750      |
|    time_elapsed     | 11       |
|    total_timesteps  | 8827     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000153 |
|    n_updates        | 2181     |
----------------------------------
Eval num_timesteps=9000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000272 |
|    n_updates        | 2224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | -2.55    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 96       |
|    fps              | 745      |
|    time_elapsed     | 12       |
|    total_timesteps  | 9215     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.15e-05 |
|    n_updates        | 2278     |
----------------------------------
Eval num_timesteps=9500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 9500     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.06e-05 |
|    n_updates        | 2349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | -2.45    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 100      |
|    fps              | 741      |
|    time_elapsed     | 12       |
|    total_timesteps  | 9603     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.18e-05 |
|    n_updates        | 2375     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | -1.1     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 104      |
|    fps              | 749      |
|    time_elapsed     | 13       |
|    total_timesteps  | 9991     |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 9.78e-05 |
|    n_updates        | 2472     |
----------------------------------
Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 10000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.26e-05 |
|    n_updates        | 2474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | -0.65    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 108      |
|    fps              | 745      |
|    time_elapsed     | 13       |
|    total_timesteps  | 10379    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 2569     |
----------------------------------
Eval num_timesteps=10500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.79e-05 |
|    n_updates        | 2599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | -0.65    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 112      |
|    fps              | 742      |
|    time_elapsed     | 14       |
|    total_timesteps  | 10758    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000154 |
|    n_updates        | 2664     |
----------------------------------
Eval num_timesteps=11000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 11000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.87e-05 |
|    n_updates        | 2724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | -0.75    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 116      |
|    fps              | 738      |
|    time_elapsed     | 15       |
|    total_timesteps  | 11146    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.06e-05 |
|    n_updates        | 2761     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | -0.3     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 120      |
|    fps              | 744      |
|    time_elapsed     | 15       |
|    total_timesteps  | 11495    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.33e-05 |
|    n_updates        | 2848     |
----------------------------------
Eval num_timesteps=11500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 11500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 2849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 124      |
|    fps              | 739      |
|    time_elapsed     | 15       |
|    total_timesteps  | 11749    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000404 |
|    n_updates        | 2912     |
----------------------------------
Eval num_timesteps=12000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 12000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000152 |
|    n_updates        | 2974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 0.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 128      |
|    fps              | 735      |
|    time_elapsed     | 16       |
|    total_timesteps  | 12122    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.29e-05 |
|    n_updates        | 3005     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.6     |
|    ep_rew_mean      | 0.4      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 132      |
|    fps              | 738      |
|    time_elapsed     | 16       |
|    total_timesteps  | 12242    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 3035     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.3     |
|    ep_rew_mean      | 0.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 136      |
|    fps              | 741      |
|    time_elapsed     | 16       |
|    total_timesteps  | 12466    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.64e-05 |
|    n_updates        | 3091     |
----------------------------------
Eval num_timesteps=12500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 12500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.05e-05 |
|    n_updates        | 3099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.3     |
|    ep_rew_mean      | 0.65     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 140      |
|    fps              | 736      |
|    time_elapsed     | 17       |
|    total_timesteps  | 12750    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.93e-05 |
|    n_updates        | 3162     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 0.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 144      |
|    fps              | 739      |
|    time_elapsed     | 17       |
|    total_timesteps  | 12888    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000732 |
|    n_updates        | 3196     |
----------------------------------
Eval num_timesteps=13000, episode_reward=5.00 +/- 0.00
Episode length: 20.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 13000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00126  |
|    n_updates        | 3224     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 1.2      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 148      |
|    fps              | 739      |
|    time_elapsed     | 17       |
|    total_timesteps  | 13049    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000352 |
|    n_updates        | 3237     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 1.4      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 152      |
|    fps              | 740      |
|    time_elapsed     | 17       |
|    total_timesteps  | 13137    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000257 |
|    n_updates        | 3259     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 1.55     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 156      |
|    fps              | 743      |
|    time_elapsed     | 17       |
|    total_timesteps  | 13287    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.36e-05 |
|    n_updates        | 3296     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 1.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 160      |
|    fps              | 745      |
|    time_elapsed     | 17       |
|    total_timesteps  | 13395    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00099  |
|    n_updates        | 3323     |
----------------------------------
Eval num_timesteps=13500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 13500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00105  |
|    n_updates        | 3349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 1.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 164      |
|    fps              | 739      |
|    time_elapsed     | 18       |
|    total_timesteps  | 13554    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000926 |
|    n_updates        | 3363     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 2.15     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 168      |
|    fps              | 741      |
|    time_elapsed     | 18       |
|    total_timesteps  | 13693    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000141 |
|    n_updates        | 3398     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.9     |
|    ep_rew_mean      | 2.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 172      |
|    fps              | 742      |
|    time_elapsed     | 18       |
|    total_timesteps  | 13773    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00097  |
|    n_updates        | 3418     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.9     |
|    ep_rew_mean      | 2.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 176      |
|    fps              | 743      |
|    time_elapsed     | 18       |
|    total_timesteps  | 13866    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000285 |
|    n_updates        | 3441     |
----------------------------------
Eval num_timesteps=14000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 14000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.371    |
|    n_updates        | 3474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.8     |
|    ep_rew_mean      | 2.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 180      |
|    fps              | 738      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14047    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 3486     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.8     |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 184      |
|    fps              | 739      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14135    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00102  |
|    n_updates        | 3508     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.2     |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 188      |
|    fps              | 741      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14262    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00326  |
|    n_updates        | 3540     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.3     |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 192      |
|    fps              | 742      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14355    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 3563     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.3     |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 196      |
|    fps              | 744      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14448    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0025   |
|    n_updates        | 3586     |
----------------------------------
Eval num_timesteps=14500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 14500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00365  |
|    n_updates        | 3599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.4     |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 200      |
|    fps              | 743      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14539    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 3609     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.2     |
|    ep_rew_mean      | 3.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 204      |
|    fps              | 744      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14616    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000954 |
|    n_updates        | 3628     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.2     |
|    ep_rew_mean      | 4.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 208      |
|    fps              | 746      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14700    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0987   |
|    n_updates        | 3649     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.4     |
|    ep_rew_mean      | 4.3      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 212      |
|    fps              | 747      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14800    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 3674     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.4     |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 216      |
|    fps              | 748      |
|    time_elapsed     | 19       |
|    total_timesteps  | 14882    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000475 |
|    n_updates        | 3695     |
----------------------------------
Eval num_timesteps=15000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 3724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | 4.6      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 220      |
|    fps              | 748      |
|    time_elapsed     | 20       |
|    total_timesteps  | 15030    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0986   |
|    n_updates        | 3732     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.6     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 224      |
|    fps              | 750      |
|    time_elapsed     | 20       |
|    total_timesteps  | 15112    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 3752     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.8     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 228      |
|    fps              | 751      |
|    time_elapsed     | 20       |
|    total_timesteps  | 15206    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 3776     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.5     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 232      |
|    fps              | 752      |
|    time_elapsed     | 20       |
|    total_timesteps  | 15296    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000837 |
|    n_updates        | 3798     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.2     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 236      |
|    fps              | 752      |
|    time_elapsed     | 20       |
|    total_timesteps  | 15383    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00389  |
|    n_updates        | 3820     |
----------------------------------
Eval num_timesteps=15500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 15500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00566  |
|    n_updates        | 3849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.9     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 240      |
|    fps              | 753      |
|    time_elapsed     | 20       |
|    total_timesteps  | 15537    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00297  |
|    n_updates        | 3859     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.4     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 244      |
|    fps              | 754      |
|    time_elapsed     | 20       |
|    total_timesteps  | 15628    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00533  |
|    n_updates        | 3881     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.4     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 248      |
|    fps              | 756      |
|    time_elapsed     | 20       |
|    total_timesteps  | 15793    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00453  |
|    n_updates        | 3923     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.4     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 252      |
|    fps              | 757      |
|    time_elapsed     | 20       |
|    total_timesteps  | 15877    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00329  |
|    n_updates        | 3944     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.1     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 256      |
|    fps              | 759      |
|    time_elapsed     | 21       |
|    total_timesteps  | 15995    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0484   |
|    n_updates        | 3973     |
----------------------------------
Eval num_timesteps=16000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 16000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00426  |
|    n_updates        | 3974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28       |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 260      |
|    fps              | 754      |
|    time_elapsed     | 21       |
|    total_timesteps  | 16195    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00368  |
|    n_updates        | 4023     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.4     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 264      |
|    fps              | 755      |
|    time_elapsed     | 21       |
|    total_timesteps  | 16291    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0599   |
|    n_updates        | 4047     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.8     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 268      |
|    fps              | 756      |
|    time_elapsed     | 21       |
|    total_timesteps  | 16372    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0119   |
|    n_updates        | 4067     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.9     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 272      |
|    fps              | 757      |
|    time_elapsed     | 21       |
|    total_timesteps  | 16458    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 4089     |
----------------------------------
Eval num_timesteps=16500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 16500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 4099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.7     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 276      |
|    fps              | 756      |
|    time_elapsed     | 21       |
|    total_timesteps  | 16538    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0195   |
|    n_updates        | 4109     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 280      |
|    fps              | 758      |
|    time_elapsed     | 21       |
|    total_timesteps  | 16633    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0718   |
|    n_updates        | 4133     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 284      |
|    fps              | 759      |
|    time_elapsed     | 22       |
|    total_timesteps  | 16724    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.118    |
|    n_updates        | 4155     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.5     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 288      |
|    fps              | 760      |
|    time_elapsed     | 22       |
|    total_timesteps  | 16812    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.066    |
|    n_updates        | 4177     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 292      |
|    fps              | 761      |
|    time_elapsed     | 22       |
|    total_timesteps  | 16893    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0157   |
|    n_updates        | 4198     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 296      |
|    fps              | 762      |
|    time_elapsed     | 22       |
|    total_timesteps  | 16986    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 4221     |
----------------------------------
Eval num_timesteps=17000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 17000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0616   |
|    n_updates        | 4224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 300      |
|    fps              | 762      |
|    time_elapsed     | 22       |
|    total_timesteps  | 17099    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.034    |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.7     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 304      |
|    fps              | 763      |
|    time_elapsed     | 22       |
|    total_timesteps  | 17190    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0195   |
|    n_updates        | 4272     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26       |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 308      |
|    fps              | 764      |
|    time_elapsed     | 22       |
|    total_timesteps  | 17299    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 4299     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26       |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 312      |
|    fps              | 765      |
|    time_elapsed     | 22       |
|    total_timesteps  | 17398    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 4324     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 316      |
|    fps              | 766      |
|    time_elapsed     | 22       |
|    total_timesteps  | 17477    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.184    |
|    n_updates        | 4344     |
----------------------------------
Eval num_timesteps=17500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 17500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0508   |
|    n_updates        | 4349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 320      |
|    fps              | 766      |
|    time_elapsed     | 22       |
|    total_timesteps  | 17566    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0326   |
|    n_updates        | 4366     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 324      |
|    fps              | 767      |
|    time_elapsed     | 23       |
|    total_timesteps  | 17655    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.223    |
|    n_updates        | 4388     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 328      |
|    fps              | 768      |
|    time_elapsed     | 23       |
|    total_timesteps  | 17745    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 4411     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 332      |
|    fps              | 769      |
|    time_elapsed     | 23       |
|    total_timesteps  | 17840    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0534   |
|    n_updates        | 4434     |
----------------------------------
Eval num_timesteps=18000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0735   |
|    n_updates        | 4474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.5     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 336      |
|    fps              | 769      |
|    time_elapsed     | 23       |
|    total_timesteps  | 18029    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 4482     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26       |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 340      |
|    fps              | 771      |
|    time_elapsed     | 23       |
|    total_timesteps  | 18134    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0584   |
|    n_updates        | 4508     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 344      |
|    fps              | 771      |
|    time_elapsed     | 23       |
|    total_timesteps  | 18214    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 4528     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | 5        |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 348      |
|    fps              | 772      |
|    time_elapsed     | 23       |
|    total_timesteps  | 18313    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.172    |
|    n_updates        | 4553     |
----------------------------------
Eval num_timesteps=18500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.206    |
|    n_updates        | 4599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.6     |
|    ep_rew_mean      | 5        |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 352      |
|    fps              | 768      |
|    time_elapsed     | 24       |
|    total_timesteps  | 18538    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.206    |
|    n_updates        | 4609     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.9     |
|    ep_rew_mean      | 5        |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 356      |
|    fps              | 770      |
|    time_elapsed     | 24       |
|    total_timesteps  | 18690    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0204   |
|    n_updates        | 4647     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.2     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 360      |
|    fps              | 772      |
|    time_elapsed     | 24       |
|    total_timesteps  | 18915    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0374   |
|    n_updates        | 4703     |
----------------------------------
Eval num_timesteps=19000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 19000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0562   |
|    n_updates        | 4724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.3     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 364      |
|    fps              | 769      |
|    time_elapsed     | 24       |
|    total_timesteps  | 19225    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0335   |
|    n_updates        | 4781     |
----------------------------------
Eval num_timesteps=19500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 19500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0454   |
|    n_updates        | 4849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 368      |
|    fps              | 766      |
|    time_elapsed     | 25       |
|    total_timesteps  | 19562    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0399   |
|    n_updates        | 4865     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.9     |
|    ep_rew_mean      | 4.6      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 372      |
|    fps              | 770      |
|    time_elapsed     | 25       |
|    total_timesteps  | 19950    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 4962     |
----------------------------------
Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.215    |
|    n_updates        | 4974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.9     |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 376      |
|    fps              | 767      |
|    time_elapsed     | 26       |
|    total_timesteps  | 20329    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 5057     |
----------------------------------
Eval num_timesteps=20500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 20500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 5099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.6     |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 380      |
|    fps              | 765      |
|    time_elapsed     | 27       |
|    total_timesteps  | 20692    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0157   |
|    n_updates        | 5147     |
----------------------------------
Eval num_timesteps=21000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 21000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 5224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.4     |
|    ep_rew_mean      | 4.2      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 384      |
|    fps              | 763      |
|    time_elapsed     | 27       |
|    total_timesteps  | 21061    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00997  |
|    n_updates        | 5240     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.4     |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 388      |
|    fps              | 766      |
|    time_elapsed     | 27       |
|    total_timesteps  | 21449    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0309   |
|    n_updates        | 5337     |
----------------------------------
Eval num_timesteps=21500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 21500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0518   |
|    n_updates        | 5349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.4     |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 392      |
|    fps              | 764      |
|    time_elapsed     | 28       |
|    total_timesteps  | 21837    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00488  |
|    n_updates        | 5434     |
----------------------------------
Eval num_timesteps=22000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 22000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0558   |
|    n_updates        | 5474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.4     |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 396      |
|    fps              | 762      |
|    time_elapsed     | 29       |
|    total_timesteps  | 22225    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0128   |
|    n_updates        | 5531     |
----------------------------------
Eval num_timesteps=22500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 22500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 5599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.1     |
|    ep_rew_mean      | 3.4      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 400      |
|    fps              | 760      |
|    time_elapsed     | 29       |
|    total_timesteps  | 22613    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.203    |
|    n_updates        | 5628     |
----------------------------------
Eval num_timesteps=23000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 23000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00736  |
|    n_updates        | 5724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.1     |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 404      |
|    fps              | 758      |
|    time_elapsed     | 30       |
|    total_timesteps  | 23001    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 5725     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.9     |
|    ep_rew_mean      | 3        |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 408      |
|    fps              | 762      |
|    time_elapsed     | 30       |
|    total_timesteps  | 23389    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00543  |
|    n_updates        | 5822     |
----------------------------------
Eval num_timesteps=23500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 23500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.189    |
|    n_updates        | 5849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.8     |
|    ep_rew_mean      | 2.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 412      |
|    fps              | 760      |
|    time_elapsed     | 31       |
|    total_timesteps  | 23777    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00591  |
|    n_updates        | 5919     |
----------------------------------
Eval num_timesteps=24000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 24000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00843  |
|    n_updates        | 5974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.9     |
|    ep_rew_mean      | 2.6      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 416      |
|    fps              | 758      |
|    time_elapsed     | 31       |
|    total_timesteps  | 24165    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00274  |
|    n_updates        | 6016     |
----------------------------------
Eval num_timesteps=24500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 24500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00367  |
|    n_updates        | 6099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.9     |
|    ep_rew_mean      | 2.4      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 420      |
|    fps              | 757      |
|    time_elapsed     | 32       |
|    total_timesteps  | 24553    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 6113     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 2.2      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 424      |
|    fps              | 760      |
|    time_elapsed     | 32       |
|    total_timesteps  | 24941    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.21     |
|    n_updates        | 6210     |
----------------------------------
Eval num_timesteps=25000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 6224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 2        |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 428      |
|    fps              | 758      |
|    time_elapsed     | 33       |
|    total_timesteps  | 25329    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0944   |
|    n_updates        | 6307     |
----------------------------------
Eval num_timesteps=25500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 25500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00562  |
|    n_updates        | 6349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 1.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 432      |
|    fps              | 757      |
|    time_elapsed     | 33       |
|    total_timesteps  | 25717    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.216    |
|    n_updates        | 6404     |
----------------------------------
Eval num_timesteps=26000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 26000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00223  |
|    n_updates        | 6474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 1.6      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 436      |
|    fps              | 755      |
|    time_elapsed     | 34       |
|    total_timesteps  | 26105    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00752  |
|    n_updates        | 6501     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.6     |
|    ep_rew_mean      | 1.4      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 440      |
|    fps              | 758      |
|    time_elapsed     | 34       |
|    total_timesteps  | 26493    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00734  |
|    n_updates        | 6598     |
----------------------------------
Eval num_timesteps=26500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 26500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00521  |
|    n_updates        | 6599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 1.15     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 444      |
|    fps              | 757      |
|    time_elapsed     | 35       |
|    total_timesteps  | 26881    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000477 |
|    n_updates        | 6695     |
----------------------------------
Eval num_timesteps=27000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0232   |
|    n_updates        | 6724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.6     |
|    ep_rew_mean      | 0.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 448      |
|    fps              | 755      |
|    time_elapsed     | 36       |
|    total_timesteps  | 27269    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00114  |
|    n_updates        | 6792     |
----------------------------------
Eval num_timesteps=27500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 27500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00272  |
|    n_updates        | 6849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.2     |
|    ep_rew_mean      | 0.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 452      |
|    fps              | 753      |
|    time_elapsed     | 36       |
|    total_timesteps  | 27657    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 6889     |
----------------------------------
Eval num_timesteps=28000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 28000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00912  |
|    n_updates        | 6974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 0.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 456      |
|    fps              | 751      |
|    time_elapsed     | 37       |
|    total_timesteps  | 28045    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00106  |
|    n_updates        | 6986     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.2     |
|    ep_rew_mean      | 0.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 460      |
|    fps              | 754      |
|    time_elapsed     | 37       |
|    total_timesteps  | 28433    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.19     |
|    n_updates        | 7083     |
----------------------------------
Eval num_timesteps=28500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 28500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000994 |
|    n_updates        | 7099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 0.25     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 464      |
|    fps              | 752      |
|    time_elapsed     | 38       |
|    total_timesteps  | 28821    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.18     |
|    n_updates        | 7180     |
----------------------------------
Eval num_timesteps=29000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 29000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.193    |
|    n_updates        | 7224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 0.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 468      |
|    fps              | 751      |
|    time_elapsed     | 38       |
|    total_timesteps  | 29209    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0013   |
|    n_updates        | 7277     |
----------------------------------
Eval num_timesteps=29500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 29500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000777 |
|    n_updates        | 7349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 0.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 472      |
|    fps              | 749      |
|    time_elapsed     | 39       |
|    total_timesteps  | 29597    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00111  |
|    n_updates        | 7374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | 0        |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 476      |
|    fps              | 752      |
|    time_elapsed     | 39       |
|    total_timesteps  | 29985    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000883 |
|    n_updates        | 7471     |
----------------------------------
Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000793 |
|    n_updates        | 7474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 480      |
|    fps              | 750      |
|    time_elapsed     | 40       |
|    total_timesteps  | 30373    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00139  |
|    n_updates        | 7568     |
----------------------------------
Eval num_timesteps=30500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 30500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00361  |
|    n_updates        | 7599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 484      |
|    fps              | 749      |
|    time_elapsed     | 41       |
|    total_timesteps  | 30761    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000741 |
|    n_updates        | 7665     |
----------------------------------
Eval num_timesteps=31000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 31000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0015   |
|    n_updates        | 7724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 488      |
|    fps              | 748      |
|    time_elapsed     | 41       |
|    total_timesteps  | 31149    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 7762     |
----------------------------------
Eval num_timesteps=31500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 31500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000924 |
|    n_updates        | 7849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 492      |
|    fps              | 746      |
|    time_elapsed     | 42       |
|    total_timesteps  | 31537    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000466 |
|    n_updates        | 7859     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 496      |
|    fps              | 749      |
|    time_elapsed     | 42       |
|    total_timesteps  | 31925    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 7956     |
----------------------------------
Eval num_timesteps=32000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 32000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000761 |
|    n_updates        | 7974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 500      |
|    fps              | 748      |
|    time_elapsed     | 43       |
|    total_timesteps  | 32313    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00138  |
|    n_updates        | 8053     |
----------------------------------
Eval num_timesteps=32500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 32500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00122  |
|    n_updates        | 8099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 504      |
|    fps              | 747      |
|    time_elapsed     | 43       |
|    total_timesteps  | 32701    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0014   |
|    n_updates        | 8150     |
----------------------------------
Eval num_timesteps=33000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 33000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00192  |
|    n_updates        | 8224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 508      |
|    fps              | 746      |
|    time_elapsed     | 44       |
|    total_timesteps  | 33089    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 8247     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 512      |
|    fps              | 748      |
|    time_elapsed     | 44       |
|    total_timesteps  | 33477    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00268  |
|    n_updates        | 8344     |
----------------------------------
Eval num_timesteps=33500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 33500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000383 |
|    n_updates        | 8349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 516      |
|    fps              | 747      |
|    time_elapsed     | 45       |
|    total_timesteps  | 33865    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0035   |
|    n_updates        | 8441     |
----------------------------------
Eval num_timesteps=34000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 34000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00144  |
|    n_updates        | 8474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 520      |
|    fps              | 746      |
|    time_elapsed     | 45       |
|    total_timesteps  | 34253    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 8538     |
----------------------------------
Eval num_timesteps=34500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 34500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00119  |
|    n_updates        | 8599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 524      |
|    fps              | 745      |
|    time_elapsed     | 46       |
|    total_timesteps  | 34616    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000815 |
|    n_updates        | 8628     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 528      |
|    fps              | 747      |
|    time_elapsed     | 46       |
|    total_timesteps  | 34988    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 8721     |
----------------------------------
Eval num_timesteps=35000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000506 |
|    n_updates        | 8724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | -0.05    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 532      |
|    fps              | 746      |
|    time_elapsed     | 47       |
|    total_timesteps  | 35326    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0666   |
|    n_updates        | 8806     |
----------------------------------
Eval num_timesteps=35500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 35500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 8849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | -0.05    |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 536      |
|    fps              | 744      |
|    time_elapsed     | 47       |
|    total_timesteps  | 35714    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00118  |
|    n_updates        | 8903     |
----------------------------------
Eval num_timesteps=36000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00107  |
|    n_updates        | 8974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 0        |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 540      |
|    fps              | 743      |
|    time_elapsed     | 48       |
|    total_timesteps  | 36043    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 8985     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 0.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 544      |
|    fps              | 745      |
|    time_elapsed     | 48       |
|    total_timesteps  | 36388    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00137  |
|    n_updates        | 9071     |
----------------------------------
Eval num_timesteps=36500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 36500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 9099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 0.3      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 548      |
|    fps              | 743      |
|    time_elapsed     | 49       |
|    total_timesteps  | 36619    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000473 |
|    n_updates        | 9129     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.6     |
|    ep_rew_mean      | 0.5      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 552      |
|    fps              | 744      |
|    time_elapsed     | 49       |
|    total_timesteps  | 36817    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000583 |
|    n_updates        | 9179     |
----------------------------------
Eval num_timesteps=37000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 37000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 9224     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.8     |
|    ep_rew_mean      | 0.55     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 556      |
|    fps              | 743      |
|    time_elapsed     | 49       |
|    total_timesteps  | 37130    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0104   |
|    n_updates        | 9257     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.5     |
|    ep_rew_mean      | 0.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 560      |
|    fps              | 744      |
|    time_elapsed     | 50       |
|    total_timesteps  | 37279    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 9294     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.4     |
|    ep_rew_mean      | 0.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 564      |
|    fps              | 745      |
|    time_elapsed     | 50       |
|    total_timesteps  | 37465    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 9341     |
----------------------------------
Eval num_timesteps=37500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 37500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00179  |
|    n_updates        | 9349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 1.15     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 568      |
|    fps              | 743      |
|    time_elapsed     | 50       |
|    total_timesteps  | 37642    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00107  |
|    n_updates        | 9385     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | 1.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 572      |
|    fps              | 744      |
|    time_elapsed     | 50       |
|    total_timesteps  | 37866    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00224  |
|    n_updates        | 9441     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 1.55     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 576      |
|    fps              | 744      |
|    time_elapsed     | 50       |
|    total_timesteps  | 37951    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000762 |
|    n_updates        | 9462     |
----------------------------------
Eval num_timesteps=38000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 38000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000447 |
|    n_updates        | 9474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 1.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 580      |
|    fps              | 742      |
|    time_elapsed     | 51       |
|    total_timesteps  | 38060    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.253    |
|    n_updates        | 9489     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 1.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 584      |
|    fps              | 743      |
|    time_elapsed     | 51       |
|    total_timesteps  | 38245    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 9536     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 2.15     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 588      |
|    fps              | 744      |
|    time_elapsed     | 51       |
|    total_timesteps  | 38336    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 9558     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.9     |
|    ep_rew_mean      | 2.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 592      |
|    fps              | 744      |
|    time_elapsed     | 51       |
|    total_timesteps  | 38431    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000854 |
|    n_updates        | 9582     |
----------------------------------
Eval num_timesteps=38500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 38500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00139  |
|    n_updates        | 9599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.2     |
|    ep_rew_mean      | 2.55     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 596      |
|    fps              | 744      |
|    time_elapsed     | 51       |
|    total_timesteps  | 38544    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 9610     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.4     |
|    ep_rew_mean      | 2.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 600      |
|    fps              | 745      |
|    time_elapsed     | 51       |
|    total_timesteps  | 38653    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0015   |
|    n_updates        | 9638     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.4     |
|    ep_rew_mean      | 2.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 604      |
|    fps              | 745      |
|    time_elapsed     | 51       |
|    total_timesteps  | 38743    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 9660     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.6     |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 608      |
|    fps              | 746      |
|    time_elapsed     | 52       |
|    total_timesteps  | 38853    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000565 |
|    n_updates        | 9688     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.8     |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 612      |
|    fps              | 746      |
|    time_elapsed     | 52       |
|    total_timesteps  | 38961    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00109  |
|    n_updates        | 9715     |
----------------------------------
Eval num_timesteps=39000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 39000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000672 |
|    n_updates        | 9724     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.5     |
|    ep_rew_mean      | 3.55     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 616      |
|    fps              | 747      |
|    time_elapsed     | 52       |
|    total_timesteps  | 39117    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 9754     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.6     |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 620      |
|    fps              | 747      |
|    time_elapsed     | 52       |
|    total_timesteps  | 39215    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00128  |
|    n_updates        | 9778     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.9     |
|    ep_rew_mean      | 3.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 624      |
|    fps              | 747      |
|    time_elapsed     | 52       |
|    total_timesteps  | 39302    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00056  |
|    n_updates        | 9800     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44       |
|    ep_rew_mean      | 4.1      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 628      |
|    fps              | 748      |
|    time_elapsed     | 52       |
|    total_timesteps  | 39386    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000385 |
|    n_updates        | 9821     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.5     |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 632      |
|    fps              | 748      |
|    time_elapsed     | 52       |
|    total_timesteps  | 39472    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.263    |
|    n_updates        | 9842     |
----------------------------------
Eval num_timesteps=39500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 39500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000482 |
|    n_updates        | 9849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.5     |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 636      |
|    fps              | 746      |
|    time_elapsed     | 53       |
|    total_timesteps  | 39565    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000475 |
|    n_updates        | 9866     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.5     |
|    ep_rew_mean      | 4.6      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 640      |
|    fps              | 746      |
|    time_elapsed     | 53       |
|    total_timesteps  | 39695    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00108  |
|    n_updates        | 9898     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.5     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 644      |
|    fps              | 747      |
|    time_elapsed     | 53       |
|    total_timesteps  | 39834    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000446 |
|    n_updates        | 9933     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33       |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 648      |
|    fps              | 747      |
|    time_elapsed     | 53       |
|    total_timesteps  | 39918    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 9954     |
----------------------------------
Eval num_timesteps=40000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000613 |
|    n_updates        | 9974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 652      |
|    fps              | 747      |
|    time_elapsed     | 53       |
|    total_timesteps  | 40009    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000456 |
|    n_updates        | 9977     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.1     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 656      |
|    fps              | 748      |
|    time_elapsed     | 53       |
|    total_timesteps  | 40135    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0509   |
|    n_updates        | 10008    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.7     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 660      |
|    fps              | 748      |
|    time_elapsed     | 53       |
|    total_timesteps  | 40250    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00434  |
|    n_updates        | 10037    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.5     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 664      |
|    fps              | 749      |
|    time_elapsed     | 53       |
|    total_timesteps  | 40416    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 10078    |
----------------------------------
Eval num_timesteps=40500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000313 |
|    n_updates        | 10099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29       |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 668      |
|    fps              | 747      |
|    time_elapsed     | 54       |
|    total_timesteps  | 40544    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000331 |
|    n_updates        | 10110    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.7     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 672      |
|    fps              | 747      |
|    time_elapsed     | 54       |
|    total_timesteps  | 40632    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00171  |
|    n_updates        | 10132    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.6     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 676      |
|    fps              | 748      |
|    time_elapsed     | 54       |
|    total_timesteps  | 40714    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000545 |
|    n_updates        | 10153    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.5     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 680      |
|    fps              | 748      |
|    time_elapsed     | 54       |
|    total_timesteps  | 40807    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.257    |
|    n_updates        | 10176    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.7     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 684      |
|    fps              | 749      |
|    time_elapsed     | 54       |
|    total_timesteps  | 40913    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000891 |
|    n_updates        | 10203    |
----------------------------------
Eval num_timesteps=41000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000904 |
|    n_updates        | 10224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.7     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 688      |
|    fps              | 749      |
|    time_elapsed     | 54       |
|    total_timesteps  | 41005    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 10226    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.6     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 692      |
|    fps              | 749      |
|    time_elapsed     | 54       |
|    total_timesteps  | 41088    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00389  |
|    n_updates        | 10246    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.2     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 696      |
|    fps              | 750      |
|    time_elapsed     | 54       |
|    total_timesteps  | 41267    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00134  |
|    n_updates        | 10291    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.6     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 700      |
|    fps              | 751      |
|    time_elapsed     | 55       |
|    total_timesteps  | 41409    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.208    |
|    n_updates        | 10327    |
----------------------------------
Eval num_timesteps=41500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000334 |
|    n_updates        | 10349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.4     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 704      |
|    fps              | 749      |
|    time_elapsed     | 55       |
|    total_timesteps  | 41583    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000335 |
|    n_updates        | 10370    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.1     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 708      |
|    fps              | 750      |
|    time_elapsed     | 55       |
|    total_timesteps  | 41764    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000417 |
|    n_updates        | 10415    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.2     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 712      |
|    fps              | 750      |
|    time_elapsed     | 55       |
|    total_timesteps  | 41883    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 10445    |
----------------------------------
Eval num_timesteps=42000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.261    |
|    n_updates        | 10474    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.2     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 716      |
|    fps              | 748      |
|    time_elapsed     | 56       |
|    total_timesteps  | 42041    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 10485    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.2     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 720      |
|    fps              | 749      |
|    time_elapsed     | 56       |
|    total_timesteps  | 42133    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 10508    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.2     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 724      |
|    fps              | 749      |
|    time_elapsed     | 56       |
|    total_timesteps  | 42225    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000348 |
|    n_updates        | 10531    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.4     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 728      |
|    fps              | 750      |
|    time_elapsed     | 56       |
|    total_timesteps  | 42325    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000378 |
|    n_updates        | 10556    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.3     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 732      |
|    fps              | 750      |
|    time_elapsed     | 56       |
|    total_timesteps  | 42405    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000256 |
|    n_updates        | 10576    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.2     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 736      |
|    fps              | 750      |
|    time_elapsed     | 56       |
|    total_timesteps  | 42490    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 10597    |
----------------------------------
Eval num_timesteps=42500, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000203 |
|    n_updates        | 10599    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.9     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 740      |
|    fps              | 748      |
|    time_elapsed     | 56       |
|    total_timesteps  | 42584    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0022   |
|    n_updates        | 10620    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.4     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 744      |
|    fps              | 748      |
|    time_elapsed     | 56       |
|    total_timesteps  | 42673    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000307 |
|    n_updates        | 10643    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.4     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 748      |
|    fps              | 749      |
|    time_elapsed     | 57       |
|    total_timesteps  | 42762    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000519 |
|    n_updates        | 10665    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.5     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 752      |
|    fps              | 749      |
|    time_elapsed     | 57       |
|    total_timesteps  | 42860    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000497 |
|    n_updates        | 10689    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.1     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 756      |
|    fps              | 750      |
|    time_elapsed     | 57       |
|    total_timesteps  | 42942    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 10710    |
----------------------------------
Eval num_timesteps=43000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0322   |
|    n_updates        | 10724    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.8     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 760      |
|    fps              | 750      |
|    time_elapsed     | 57       |
|    total_timesteps  | 43033    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000386 |
|    n_updates        | 10733    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.1     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 764      |
|    fps              | 750      |
|    time_elapsed     | 57       |
|    total_timesteps  | 43123    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0896   |
|    n_updates        | 10755    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.7     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 768      |
|    fps              | 750      |
|    time_elapsed     | 57       |
|    total_timesteps  | 43217    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00046  |
|    n_updates        | 10779    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.8     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 772      |
|    fps              | 751      |
|    time_elapsed     | 57       |
|    total_timesteps  | 43311    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00225  |
|    n_updates        | 10802    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.8     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 776      |
|    fps              | 751      |
|    time_elapsed     | 57       |
|    total_timesteps  | 43396    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 10823    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.8     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 780      |
|    fps              | 752      |
|    time_elapsed     | 57       |
|    total_timesteps  | 43487    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000818 |
|    n_updates        | 10846    |
----------------------------------
Eval num_timesteps=43500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000245 |
|    n_updates        | 10849    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.6     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 784      |
|    fps              | 751      |
|    time_elapsed     | 57       |
|    total_timesteps  | 43574    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000262 |
|    n_updates        | 10868    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.6     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 788      |
|    fps              | 752      |
|    time_elapsed     | 58       |
|    total_timesteps  | 43664    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000246 |
|    n_updates        | 10890    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.8     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 792      |
|    fps              | 752      |
|    time_elapsed     | 58       |
|    total_timesteps  | 43763    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0012   |
|    n_updates        | 10915    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.8     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 796      |
|    fps              | 753      |
|    time_elapsed     | 58       |
|    total_timesteps  | 43848    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 10936    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 800      |
|    fps              | 753      |
|    time_elapsed     | 58       |
|    total_timesteps  | 43933    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000618 |
|    n_updates        | 10958    |
----------------------------------
Eval num_timesteps=44000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000241 |
|    n_updates        | 10974    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 804      |
|    fps              | 753      |
|    time_elapsed     | 58       |
|    total_timesteps  | 44030    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00133  |
|    n_updates        | 10982    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 808      |
|    fps              | 753      |
|    time_elapsed     | 58       |
|    total_timesteps  | 44127    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00145  |
|    n_updates        | 11006    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 812      |
|    fps              | 754      |
|    time_elapsed     | 58       |
|    total_timesteps  | 44220    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000439 |
|    n_updates        | 11029    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 816      |
|    fps              | 754      |
|    time_elapsed     | 58       |
|    total_timesteps  | 44308    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 11051    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 820      |
|    fps              | 754      |
|    time_elapsed     | 58       |
|    total_timesteps  | 44398    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000357 |
|    n_updates        | 11074    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 824      |
|    fps              | 755      |
|    time_elapsed     | 58       |
|    total_timesteps  | 44491    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000731 |
|    n_updates        | 11097    |
----------------------------------
Eval num_timesteps=44500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00264  |
|    n_updates        | 11099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 828      |
|    fps              | 755      |
|    time_elapsed     | 59       |
|    total_timesteps  | 44595    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000445 |
|    n_updates        | 11123    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 832      |
|    fps              | 755      |
|    time_elapsed     | 59       |
|    total_timesteps  | 44685    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000268 |
|    n_updates        | 11146    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 836      |
|    fps              | 756      |
|    time_elapsed     | 59       |
|    total_timesteps  | 44782    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000369 |
|    n_updates        | 11170    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 840      |
|    fps              | 756      |
|    time_elapsed     | 59       |
|    total_timesteps  | 44871    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000503 |
|    n_updates        | 11192    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 844      |
|    fps              | 756      |
|    time_elapsed     | 59       |
|    total_timesteps  | 44956    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 11213    |
----------------------------------
Eval num_timesteps=45000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00031  |
|    n_updates        | 11224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 848      |
|    fps              | 756      |
|    time_elapsed     | 59       |
|    total_timesteps  | 45038    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0919   |
|    n_updates        | 11234    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 852      |
|    fps              | 757      |
|    time_elapsed     | 59       |
|    total_timesteps  | 45126    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000428 |
|    n_updates        | 11256    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 856      |
|    fps              | 757      |
|    time_elapsed     | 59       |
|    total_timesteps  | 45214    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000557 |
|    n_updates        | 11278    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 860      |
|    fps              | 757      |
|    time_elapsed     | 59       |
|    total_timesteps  | 45316    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0209   |
|    n_updates        | 11303    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 864      |
|    fps              | 758      |
|    time_elapsed     | 59       |
|    total_timesteps  | 45412    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000267 |
|    n_updates        | 11327    |
----------------------------------
Eval num_timesteps=45500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000337 |
|    n_updates        | 11349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 868      |
|    fps              | 758      |
|    time_elapsed     | 60       |
|    total_timesteps  | 45500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 872      |
|    fps              | 758      |
|    time_elapsed     | 60       |
|    total_timesteps  | 45587    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000434 |
|    n_updates        | 11371    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 876      |
|    fps              | 758      |
|    time_elapsed     | 60       |
|    total_timesteps  | 45676    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000727 |
|    n_updates        | 11393    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 880      |
|    fps              | 759      |
|    time_elapsed     | 60       |
|    total_timesteps  | 45759    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 11414    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 884      |
|    fps              | 759      |
|    time_elapsed     | 60       |
|    total_timesteps  | 45862    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000429 |
|    n_updates        | 11440    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 888      |
|    fps              | 760      |
|    time_elapsed     | 60       |
|    total_timesteps  | 45945    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 11461    |
----------------------------------
Eval num_timesteps=46000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00036  |
|    n_updates        | 11474    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 892      |
|    fps              | 759      |
|    time_elapsed     | 60       |
|    total_timesteps  | 46036    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0012   |
|    n_updates        | 11483    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 896      |
|    fps              | 760      |
|    time_elapsed     | 60       |
|    total_timesteps  | 46110    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000883 |
|    n_updates        | 11502    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 900      |
|    fps              | 760      |
|    time_elapsed     | 60       |
|    total_timesteps  | 46201    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0941   |
|    n_updates        | 11525    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 904      |
|    fps              | 761      |
|    time_elapsed     | 60       |
|    total_timesteps  | 46286    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000207 |
|    n_updates        | 11546    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 908      |
|    fps              | 761      |
|    time_elapsed     | 60       |
|    total_timesteps  | 46376    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000121 |
|    n_updates        | 11568    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 912      |
|    fps              | 761      |
|    time_elapsed     | 60       |
|    total_timesteps  | 46470    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 11592    |
----------------------------------
Eval num_timesteps=46500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00018  |
|    n_updates        | 11599    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 916      |
|    fps              | 761      |
|    time_elapsed     | 61       |
|    total_timesteps  | 46569    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000631 |
|    n_updates        | 11617    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 920      |
|    fps              | 762      |
|    time_elapsed     | 61       |
|    total_timesteps  | 46644    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000436 |
|    n_updates        | 11635    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 924      |
|    fps              | 762      |
|    time_elapsed     | 61       |
|    total_timesteps  | 46738    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000169 |
|    n_updates        | 11659    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 928      |
|    fps              | 762      |
|    time_elapsed     | 61       |
|    total_timesteps  | 46831    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000421 |
|    n_updates        | 11682    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 932      |
|    fps              | 763      |
|    time_elapsed     | 61       |
|    total_timesteps  | 46913    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00015  |
|    n_updates        | 11703    |
----------------------------------
Eval num_timesteps=47000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00134  |
|    n_updates        | 11724    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.2     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 936      |
|    fps              | 763      |
|    time_elapsed     | 61       |
|    total_timesteps  | 47000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.2     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 940      |
|    fps              | 763      |
|    time_elapsed     | 61       |
|    total_timesteps  | 47089    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00026  |
|    n_updates        | 11747    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.2     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 944      |
|    fps              | 763      |
|    time_elapsed     | 61       |
|    total_timesteps  | 47178    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 11769    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 948      |
|    fps              | 764      |
|    time_elapsed     | 61       |
|    total_timesteps  | 47269    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000449 |
|    n_updates        | 11792    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 952      |
|    fps              | 764      |
|    time_elapsed     | 61       |
|    total_timesteps  | 47356    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 11813    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 956      |
|    fps              | 764      |
|    time_elapsed     | 62       |
|    total_timesteps  | 47446    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000833 |
|    n_updates        | 11836    |
----------------------------------
Eval num_timesteps=47500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 11849    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.1     |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 960      |
|    fps              | 764      |
|    time_elapsed     | 62       |
|    total_timesteps  | 47531    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000536 |
|    n_updates        | 11857    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 964      |
|    fps              | 765      |
|    time_elapsed     | 62       |
|    total_timesteps  | 47612    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00203  |
|    n_updates        | 11877    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.1     |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 968      |
|    fps              | 765      |
|    time_elapsed     | 62       |
|    total_timesteps  | 47706    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000688 |
|    n_updates        | 11901    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 972      |
|    fps              | 765      |
|    time_elapsed     | 62       |
|    total_timesteps  | 47790    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 11922    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 976      |
|    fps              | 766      |
|    time_elapsed     | 62       |
|    total_timesteps  | 47873    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000343 |
|    n_updates        | 11943    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 980      |
|    fps              | 766      |
|    time_elapsed     | 62       |
|    total_timesteps  | 47963    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00556  |
|    n_updates        | 11965    |
----------------------------------
Eval num_timesteps=48000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000141 |
|    n_updates        | 11974    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 984      |
|    fps              | 766      |
|    time_elapsed     | 62       |
|    total_timesteps  | 48044    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00031  |
|    n_updates        | 11985    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 988      |
|    fps              | 766      |
|    time_elapsed     | 62       |
|    total_timesteps  | 48126    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00256  |
|    n_updates        | 12006    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 992      |
|    fps              | 767      |
|    time_elapsed     | 62       |
|    total_timesteps  | 48236    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000467 |
|    n_updates        | 12033    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.1     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 996      |
|    fps              | 767      |
|    time_elapsed     | 62       |
|    total_timesteps  | 48320    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 12054    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.2     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 767      |
|    time_elapsed     | 63       |
|    total_timesteps  | 48419    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00024  |
|    n_updates        | 12079    |
----------------------------------
Eval num_timesteps=48500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000606 |
|    n_updates        | 12099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 767      |
|    time_elapsed     | 63       |
|    total_timesteps  | 48515    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00038  |
|    n_updates        | 12103    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 768      |
|    time_elapsed     | 63       |
|    total_timesteps  | 48603    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000573 |
|    n_updates        | 12125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 768      |
|    time_elapsed     | 63       |
|    total_timesteps  | 48709    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 12152    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 768      |
|    time_elapsed     | 63       |
|    total_timesteps  | 48797    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00042  |
|    n_updates        | 12174    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 769      |
|    time_elapsed     | 63       |
|    total_timesteps  | 48898    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000964 |
|    n_updates        | 12199    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 769      |
|    time_elapsed     | 63       |
|    total_timesteps  | 48986    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000388 |
|    n_updates        | 12221    |
----------------------------------
Eval num_timesteps=49000, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000289 |
|    n_updates        | 12224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 769      |
|    time_elapsed     | 63       |
|    total_timesteps  | 49087    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 12246    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 769      |
|    time_elapsed     | 63       |
|    total_timesteps  | 49178    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000637 |
|    n_updates        | 12269    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 770      |
|    time_elapsed     | 63       |
|    total_timesteps  | 49265    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000416 |
|    n_updates        | 12291    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 770      |
|    time_elapsed     | 64       |
|    total_timesteps  | 49346    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000161 |
|    n_updates        | 12311    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 770      |
|    time_elapsed     | 64       |
|    total_timesteps  | 49442    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 12335    |
----------------------------------
Eval num_timesteps=49500, episode_reward=5.00 +/- 0.00
Episode length: 18.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000121 |
|    n_updates        | 12349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 770      |
|    time_elapsed     | 64       |
|    total_timesteps  | 49533    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0792   |
|    n_updates        | 12358    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 771      |
|    time_elapsed     | 64       |
|    total_timesteps  | 49621    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.265    |
|    n_updates        | 12380    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 771      |
|    time_elapsed     | 64       |
|    total_timesteps  | 49715    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 12403    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 772      |
|    time_elapsed     | 64       |
|    total_timesteps  | 49823    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000249 |
|    n_updates        | 12430    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.3     |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 772      |
|    time_elapsed     | 64       |
|    total_timesteps  | 49943    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000421 |
|    n_updates        | 12460    |
----------------------------------
Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00
Episode length: 97.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 12474    |
----------------------------------
 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50,000/50,000  [ 0:01:04 < 0:00:00 , 784 it/s ]
1066
1066
Agent: [0 0]
Target: [9 9]
Action: 3
location: [1 0]
Action: 0
location: [1 1]
Action: 3
location: [2 1]
Action: 0
location: [2 2]
Action: 1
location: [1 2]
Action: 0
location: [1 3]
Action: 0
location: [1 4]
Action: 0
location: [1 5]
Action: 0
location: [1 6]
Action: 3
location: [2 6]
Action: 0
location: [2 7]
Action: 0
location: [2 8]
Action: 3
location: [3 8]
Action: 3
location: [4 8]
Action: 3
location: [5 8]
Action: 3
location: [6 8]
Action: 0
location: [6 9]
Action: 3
location: [7 9]
Action: 3
location: [8 9]
Action: 3
location: [9 9]
Completed in 19 steps with score of 5
(9, 9)
(9, 9)
['start     ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['explored  ', 'explored  ', 'explored  ', 'explored  ', 'explored  ', 'explored  ', 'explored  ', '          ', '          ', '          ']
['          ', 'explored  ', 'explored  ', '          ', '          ', '          ', 'explored  ', 'explored  ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', 'explored  ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'agent     ']
(9, 9)
(9, 9)
['start     ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ']
['explored  ', 'explored  ', 'explored  ', 'explored  ', 'explored  ', 'explored  ', 'explored  ', '          ', '          ', '          ']
['          ', 'explored  ', 'explored  ', '          ', '          ', '          ', 'explored  ', 'explored  ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', '          ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ', 'explored  ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'explored  ']
['          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', '          ', 'agent     ']
Title: dqn path
Save path: ./graphs/dqnpath.pngdqn path
Execution time: 88.86462759971619 seconds
